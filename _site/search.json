[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Visual Analytics and Applications",
    "section": "",
    "text": "Welcome to my page.\n\n\n\n\n\n\nGood morning, and in case I don’t see you,\n\n\nGood afternoon, Good evening, And good night.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHand-on EX03\n\n\n\nZHANG Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nHand-on Exercise 1\n\n\n\nChenbin Zhang\n\n\n\n\n\n\n\n\n\n\n\n\nHand-on_EX02\n\n\n\nZhang Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on EX07\n\n\n\nZhang Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on_Ex04\n\n\n\nZhang Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on_Ex05\n\n\n\nZhang Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nHands-on_Ex06\n\n\n\nZhang Chenbin\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\nIn-class EX02\n\n\n\nZHANG Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nIn-class Ex01\n\n\n\nChenbin Zhang\n\n\nApr 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nIn-class_EX04\n\n\n\nZhang Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nIn-class_EX06\n\n\n\nZhnag Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nIn-class_EX09\n\n\n\nZHANG Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nIn-class_Ex05\n\n\n\nZhang Chenbin\n\n\nMay 11, 2024\n\n\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n\n\nTake-home Exercise 1\n\n\n\nZhang Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home Exercise 3:Mini-Challenge 3\n\n\n\nZhang Chenbin\n\n\n\n\n\n\n\n\n\n\n\n\nTake-home_Ex02\n\n\n\nZhang Chenbin\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "Hello,today we choose the analysis from ZOU JIAXUN.I’ll criticize you.\n\n\n\n\n\nThis scatter plot, which displays the relationship between property area (in square feet) and transacted prices (in thousands of dollars), is created from a randomly sampled subset of 500 data points from a comprehensive dataset, rather than isolating data from the first quarter of 2024. This approach may not provide an accurate reflection of the specific market conditions of early 2024, as it includes data from various time periods with potentially different economic influences. Additionally, the plot does not differentiate between types of sales, such as new homes, resales, or foreclosures, which can vary significantly in pricing dynamics. This aggregation could obscure distinct patterns and trends that are more apparent when analyzing these groups separately, leading to less precise insights for targeted strategic decision-making or market analysis.\n\n\n\n\nFirst import the packages used for the original analysis and the packages required for the new analysis.\n\npacman::p_load(dplyr, purrr, readr, ggiraph,\n               ggplot2, lubridate, ggrepel,\n               patchwork, ggthemes, hrbrthemes, tidyverse, plotly, readr)\n\n\n\n\n\ndata1 = read_csv(\"data/ResidentialTransaction20240308160536.csv\")\ndata2 = read_csv(\"data/ResidentialTransaction20240308160736.csv\")\ndata3 = read_csv(\"data/ResidentialTransaction20240308161009.csv\")\ndata4 = read_csv(\"data/ResidentialTransaction20240308161109.csv\")\ndata5 = read_csv(\"data/ResidentialTransaction20240414220633.csv\")\ncombined_data &lt;- bind_rows(data1, data2, data3, data4, data5)\n\nglimpse(combined_data)\n\nRows: 26,806\nColumns: 21\n$ `Project Name`                &lt;chr&gt; \"THE REEF AT KING'S DOCK\", \"URBAN TREASU…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2317000, 1823500, 1421112, 1258112, 1280…\n$ `Area (SQFT)`                 &lt;dbl&gt; 882.65, 882.65, 1076.40, 1033.34, 871.88…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2625, 2066, 1320, 1218, 1468, 1767, 1095…\n$ `Sale Date`                   &lt;chr&gt; \"01 Jan 2023\", \"02 Jan 2023\", \"02 Jan 20…\n$ Address                       &lt;chr&gt; \"12 HARBOURFRONT AVENUE #05-32\", \"205 JA…\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Strata\", \"Strata\", \"Strata\", …\n$ `Area (SQM)`                  &lt;dbl&gt; 82.0, 82.0, 100.0, 96.0, 81.0, 308.7, 42…\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 28256, 22238, 14211, 13105, 15802, 19015…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Condominium\", \"Executive…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"99 yrs from 12/01/2021\", \"Freehold\", \"9…\n$ `Completion Date`             &lt;chr&gt; \"Uncompleted\", \"Uncompleted\", \"Uncomplet…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"HDB\", \"Private\", \"HDB\", \"HDB\", \"HDB\", \"…\n$ `Postal Code`                 &lt;chr&gt; \"097996\", \"419535\", \"269343\", \"269294\", …\n$ `Postal District`             &lt;chr&gt; \"04\", \"14\", \"27\", \"27\", \"28\", \"19\", \"10\"…\n$ `Postal Sector`               &lt;chr&gt; \"09\", \"41\", \"26\", \"26\", \"79\", \"54\", \"27\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"East Region\", \"North …\n$ `Planning Area`               &lt;chr&gt; \"Bukit Merah\", \"Bedok\", \"Yishun\", \"Yishu…\n\n\n\n\n\nClean and process data\n\nlibrary(dplyr)\nlibrary(lubridate)\n\ndata_cleaned &lt;- combined_data %&gt;%\n  mutate(`Sale Date` = dmy(`Sale Date`)) %&gt;%\n  mutate(`Area (SQM)` = as.numeric(gsub(\",\", \"\", `Area (SQM)`))) %&gt;%\n  mutate(`Unit Price ($ PSM)` = as.numeric(gsub(\"\\\\$\", \"\", gsub(\",\", \"\", `Unit Price ($ PSM)`)))) %&gt;%\n  mutate(`Area (SQM)` = ifelse(is.na(`Area (SQM)`), 0, `Area (SQM)`)) %&gt;%\n  mutate(Area_Category = cut(\n    `Area (SQM)`,\n    breaks = c(0, 100, 200, 300, 400, Inf),\n    labels = c(\"&lt;100\", \"100-200\", \"200-300\", \"300-400\", \"&gt;400\"),\n    include.lowest = TRUE\n  ))\n\ndata_cleaned &lt;- data_cleaned %&gt;%\n  mutate(Quarter = paste(year(`Sale Date`), \n                         ifelse(month(`Sale Date`) %in% c(1, 2, 3), \"Q1\",\n                                ifelse(month(`Sale Date`) %in% c(4, 5, 6), \"Q2\",\n                                       ifelse(month(`Sale Date`) %in% c(7, 8, 9), \"Q3\", \"Q4\"))), sep = \"-\"))\n\nstr(data_cleaned)\n\ntibble [26,806 × 23] (S3: tbl_df/tbl/data.frame)\n $ Project Name               : chr [1:26806] \"THE REEF AT KING'S DOCK\" \"URBAN TREASURES\" \"NORTH GAIA\" \"NORTH GAIA\" ...\n $ Transacted Price ($)       : num [1:26806] 2317000 1823500 1421112 1258112 1280000 ...\n $ Area (SQFT)                : num [1:26806] 883 883 1076 1033 872 ...\n $ Unit Price ($ PSF)         : num [1:26806] 2625 2066 1320 1218 1468 ...\n $ Sale Date                  : Date[1:26806], format: \"2023-01-01\" \"2023-01-02\" ...\n $ Address                    : chr [1:26806] \"12 HARBOURFRONT AVENUE #05-32\" \"205 JALAN EUNOS #08-02\" \"29 YISHUN CLOSE #08-10\" \"45 YISHUN CLOSE #07-42\" ...\n $ Type of Sale               : chr [1:26806] \"New Sale\" \"New Sale\" \"New Sale\" \"New Sale\" ...\n $ Type of Area               : chr [1:26806] \"Strata\" \"Strata\" \"Strata\" \"Strata\" ...\n $ Area (SQM)                 : num [1:26806] 82 82 100 96 81 ...\n $ Unit Price ($ PSM)         : num [1:26806] 28256 22238 14211 13105 15802 ...\n $ Nett Price($)              : chr [1:26806] \"-\" \"-\" \"-\" \"-\" ...\n $ Property Type              : chr [1:26806] \"Condominium\" \"Condominium\" \"Executive Condominium\" \"Executive Condominium\" ...\n $ Number of Units            : num [1:26806] 1 1 1 1 1 1 1 1 1 1 ...\n $ Tenure                     : chr [1:26806] \"99 yrs from 12/01/2021\" \"Freehold\" \"99 yrs from 15/02/2021\" \"99 yrs from 15/02/2021\" ...\n $ Completion Date            : chr [1:26806] \"Uncompleted\" \"Uncompleted\" \"Uncompleted\" \"Uncompleted\" ...\n $ Purchaser Address Indicator: chr [1:26806] \"HDB\" \"Private\" \"HDB\" \"HDB\" ...\n $ Postal Code                : chr [1:26806] \"097996\" \"419535\" \"269343\" \"269294\" ...\n $ Postal District            : chr [1:26806] \"04\" \"14\" \"27\" \"27\" ...\n $ Postal Sector              : chr [1:26806] \"09\" \"41\" \"26\" \"26\" ...\n $ Planning Region            : chr [1:26806] \"Central Region\" \"East Region\" \"North Region\" \"North Region\" ...\n $ Planning Area              : chr [1:26806] \"Bukit Merah\" \"Bedok\" \"Yishun\" \"Yishun\" ...\n $ Area_Category              : Factor w/ 5 levels \"&lt;100\",\"100-200\",..: 1 1 1 1 1 4 5 2 1 2 ...\n $ Quarter                    : chr [1:26806] \"2023-Q1\" \"2023-Q1\" \"2023-Q1\" \"2023-Q1\" ...\n\n\nValidate data according to original methods\n\nCodeResult\n\n\n\nduplicate &lt;- combined_data %&gt;% \n  group_by_all() %&gt;% \n  filter(n()&gt;1) %&gt;% \n  ungroup()\n\n\n\n\nduplicate\n\n# A tibble: 0 × 21\n# ℹ 21 variables: Project Name &lt;chr&gt;, Transacted Price ($) &lt;dbl&gt;,\n#   Area (SQFT) &lt;dbl&gt;, Unit Price ($ PSF) &lt;dbl&gt;, Sale Date &lt;chr&gt;,\n#   Address &lt;chr&gt;, Type of Sale &lt;chr&gt;, Type of Area &lt;chr&gt;, Area (SQM) &lt;dbl&gt;,\n#   Unit Price ($ PSM) &lt;dbl&gt;, Nett Price($) &lt;chr&gt;, Property Type &lt;chr&gt;,\n#   Number of Units &lt;dbl&gt;, Tenure &lt;chr&gt;, Completion Date &lt;chr&gt;,\n#   Purchaser Address Indicator &lt;chr&gt;, Postal Code &lt;chr&gt;,\n#   Postal District &lt;chr&gt;, Postal Sector &lt;chr&gt;, Planning Region &lt;chr&gt;, …\n\n\n\n\n\n\nCodePlot\n\n\n\nF1 &lt;- ggplot(combined_data, aes(x = `Property Type`)) + \n    geom_bar_interactive(aes(fill = `Planning Region`), position = \"dodge\") +  \n    labs(x = \"Property Type\", y = \"Frequency\",\n         title = \"Frequency of Property Types by Planning Region\") +\n    facet_wrap(~ `Planning Region`, scales = \"free\") +\n    theme_stata(base_size = 2.5)\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\nList of 1\n $ axis.text.x:List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : num 45\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\n    F1\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe absence of data from the previous year’s four quarters in the scatter plot analyzing the relationship between transacted prices and area poses certain challenges. Primarily, it limits the depth of market trend analysis over an extended period, potentially overlooking seasonal fluctuations and economic cycles that significantly influence real estate markets. Without this broader temporal context, the analysis might give undue emphasis to short-term variations or isolated events, potentially leading to a somewhat narrow perspective on market dynamics. Such a limitation curtails the capacity to observe and interpret long-term trends and the cyclical nature of the housing market, which are essential for comprehensive statistical analysis and forecasting.\nFurthermore, the lack of previous year data restricts year-over-year comparative analysis, crucial for evaluating market condition changes and understanding the influence of external economic factors like interest rate shifts or broader economic changes. For investors and policymakers, the unavailability of a more extended data range may hinder informed strategic decision-making. It limits the development of predictive models that rely on historical data to accurately project future market behaviors, thereby subtly reducing the analysis’s overall utility in supporting nuanced business and investment decisions.\nTo address the challenges of a limited temporal analysis, I have enhanced the dataset by incorporating data from the previous year and categorizing it by quarters. This addition allows for a more nuanced exploration of the changes and trends over time. By analyzing the data on a quarterly basis, we can better understand the seasonal fluctuations and the impact of economic cycles on the real estate market.\n\nCodePlotConclusion\n\n\n\nlibrary(plotly)\n\nP3_interactive &lt;- plot_ly(data = data_cleaned, x = ~`Area (SQFT)`, y = ~`Transacted Price ($)`, type = 'scatter', mode = 'markers',\n                          hoverinfo = 'text',\n                          transforms = list(\n                            list(\n                              type = 'filter',\n                              target = ~Quarter,\n                              operation = '=',\n                              value = unique(data_cleaned$Quarter)[1]  \n                            )\n                          ),\n                          text = ~paste(\"Price: \", `Transacted Price ($)`, \"&lt;br&gt;Area: \", `Area (SQFT)`, \"&lt;br&gt;Quarter: \", Quarter)) %&gt;%\n  layout(title = 'Scatter Plot of Transacted Price vs. Area',\n         xaxis = list(title = 'Area (SQFT)'),\n         yaxis = list(title = 'Transacted Price ($)'),\n         sliders = list(list(\n           active = 0,\n           currentvalue = list(prefix = \"Quarter: \"),\n           steps = lapply(unique(data_cleaned$Quarter), function(q) {\n             list(label = q, method = \"restyle\", args = list(\"transforms[0].value\", q))\n           })\n         )))\n\n\n\n\nP3_interactive\n\n\n\n\n\n\n\nThe interactive scatter plot offers several advantages over the static scatter plot with a trend line, particularly in terms of user engagement and analytical depth. Firstly, its interactivity enhances user experience by allowing for the exploration of individual data points through tooltips and dynamic elements such as sliders for temporal data filtering. This interactivity not only makes the data more accessible but also enables users to perform on-the-fly analysis of specific time periods without the need to switch between different plots. Moreover, the ability to zoom in and navigate through dense clusters of data points helps in examining detailed patterns that might be obscured in a static plot. This dynamic functionality is especially valuable in digital platforms where users expect an interactive and engaging data visualization experience.\n\n\n\n\n\n\nFailing to categorize data by “Type of Sale” in an analysis of real estate transactions introduces significant limitations that can obscure crucial insights into different market segments. Without this categorization, the analysis merges various types of transactions, such as new sales, resales, and sub-sales, into a single aggregated dataset. This aggregation can lead to a generalized overview that fails to capture the distinct behaviors and trends associated with each sale type.\nFirstly, different sale types often exhibit unique pricing patterns, demand cycles, and buyer preferences. For instance, new sales might be influenced by developer promotions and economic incentives, while resales are impacted more by the existing housing market conditions. Sub-sales, involving properties sold before their construction completion, might fluctuate based on speculative market sentiments. Without distinguishing these types, strategic decision-making becomes challenging as the nuanced dynamics of the market are not adequately represented.\nMoreover, policies and marketing strategies tailored to specific sale types cannot be effectively formulated or implemented without a clear understanding of the particular characteristics and needs of each segment. For example, marketing strategies that are effective for new developments might not work for resales. Therefore, by not categorizing data by “Type of Sale,” the analysis loses the potential to guide targeted interventions and optimize resource allocation, potentially leading to less effective strategies and missed opportunities in the market.\nTo enhance our analysis, I’ve processed the data to categorize it by different “Type of Sale.” This categorization allows us to delve into the specific characteristics and trends of new sales, resales, and sub-sales separately.\n\nCodePlotConclusion\n\n\n\nlibrary(plotly)\n\nP3_interactive &lt;- plot_ly(data = data_cleaned, x = ~`Area (SQFT)`, y = ~`Transacted Price ($)`, type = 'scatter', mode = 'markers',\n                          hoverinfo = 'text',\n                          transforms = list(\n                            list(\n                              type = 'filter',\n                              target = ~Quarter,\n                              operation = '=',\n                              value = unique(data_cleaned$Quarter)[1] \n                            ),\n                            list(\n                              type = 'filter',\n                              target = ~`Type of Sale`,\n                              operation = '=',\n                              value = unique(data_cleaned$`Type of Sale`)[1] \n                            )\n                          ),\n                          text = ~paste(\"Price: \", `Transacted Price ($)`, \"&lt;br&gt;Area: \", `Area (SQFT)`, \"&lt;br&gt;Quarter: \", Quarter, \"&lt;br&gt;Type of Sale: \", `Type of Sale`)) %&gt;%\n  layout(title = 'Scatter Plot of Transacted Price vs. Area',\n         xaxis = list(title = 'Area (SQFT)'),\n         yaxis = list(title = 'Transacted Price ($)'),\n         updatemenus = list(\n           list(\n             type = \"dropdown\",\n             direction = \"down\",\n             showactive = TRUE,\n             buttons = lapply(unique(data_cleaned$`Type of Sale`), function(type) {\n               list(\n                 method = \"restyle\",\n                 args = list(\"transforms[1].value\", type),\n                 label = type\n               )\n             })\n           )\n         ),\n         sliders = list(list(\n           active = 0,\n           currentvalue = list(prefix = \"Quarter: \"),\n           steps = lapply(unique(data_cleaned$Quarter), function(q) {\n             list(\n               label = q,\n               method = \"restyle\",\n               args = list(\"transforms[0].value\", q)  \n             )\n           })\n         )))\n\n\n\n\nP3_interactive\n\n\n\n\n\n\n\nThe updated interactive scatter plot enhances data analysis capabilities significantly by incorporating dropdown menus for category-based filtering, such as transaction types (New Sale, Resale, Sub Sale). This feature allows users to segment the dataset on the fly, enabling a focused examination of trends and patterns specific to each category. Such granularity is invaluable for users needing to make nuanced assessments of market dynamics. Furthermore, the plot’s clear presentation even in high-density areas ensures that all data points are easily visible and distinguishable, avoiding any visual confusion and facilitating precise analysis. Enhanced interactivity, provided by the dropdown filters, improves the user experience by allowing dynamic and complex data segmentation within the visualization interface itself. This makes the plot a highly effective tool for stakeholders requiring a comprehensive and detailed view of various market conditions without the need for additional tools or programming efforts.\n\n\n\n\n\n\n\n\nURA releases flash estimate of 1st Quarter 2024 private residential property price index\nUnsold private housing stock on the rise ahead of ramp-up in new launches in 2024\nHDB resale prices rise 1.7%; private home prices up 1.5% in first quarter: Flash estimates"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#getting-started",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#getting-started",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "First import the packages used for the original analysis and the packages required for the new analysis.\n\npacman::p_load(dplyr, purrr, readr, ggiraph,\n               ggplot2, lubridate, ggrepel,\n               patchwork, ggthemes, hrbrthemes, tidyverse, plotly, readr)\n\n\n\n\n\ndata1 = read_csv(\"data/ResidentialTransaction20240308160536.csv\")\ndata2 = read_csv(\"data/ResidentialTransaction20240308160736.csv\")\ndata3 = read_csv(\"data/ResidentialTransaction20240308161009.csv\")\ndata4 = read_csv(\"data/ResidentialTransaction20240308161109.csv\")\ndata5 = read_csv(\"data/ResidentialTransaction20240414220633.csv\")\ncombined_data &lt;- bind_rows(data1, data2, data3, data4, data5)\n\nglimpse(combined_data)\n\nRows: 26,806\nColumns: 21\n$ `Project Name`                &lt;chr&gt; \"THE REEF AT KING'S DOCK\", \"URBAN TREASU…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2317000, 1823500, 1421112, 1258112, 1280…\n$ `Area (SQFT)`                 &lt;dbl&gt; 882.65, 882.65, 1076.40, 1033.34, 871.88…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2625, 2066, 1320, 1218, 1468, 1767, 1095…\n$ `Sale Date`                   &lt;chr&gt; \"01 Jan 2023\", \"02 Jan 2023\", \"02 Jan 20…\n$ Address                       &lt;chr&gt; \"12 HARBOURFRONT AVENUE #05-32\", \"205 JA…\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Strata\", \"Strata\", \"Strata\", …\n$ `Area (SQM)`                  &lt;dbl&gt; 82.0, 82.0, 100.0, 96.0, 81.0, 308.7, 42…\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 28256, 22238, 14211, 13105, 15802, 19015…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Condominium\", \"Executive…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"99 yrs from 12/01/2021\", \"Freehold\", \"9…\n$ `Completion Date`             &lt;chr&gt; \"Uncompleted\", \"Uncompleted\", \"Uncomplet…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"HDB\", \"Private\", \"HDB\", \"HDB\", \"HDB\", \"…\n$ `Postal Code`                 &lt;chr&gt; \"097996\", \"419535\", \"269343\", \"269294\", …\n$ `Postal District`             &lt;chr&gt; \"04\", \"14\", \"27\", \"27\", \"28\", \"19\", \"10\"…\n$ `Postal Sector`               &lt;chr&gt; \"09\", \"41\", \"26\", \"26\", \"79\", \"54\", \"27\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"East Region\", \"North …\n$ `Planning Area`               &lt;chr&gt; \"Bukit Merah\", \"Bedok\", \"Yishun\", \"Yishu…\n\n\n\n\n\nClean and process data\n\nlibrary(dplyr)\nlibrary(lubridate)\n\ndata_cleaned &lt;- combined_data %&gt;%\n  mutate(`Sale Date` = dmy(`Sale Date`)) %&gt;%\n  mutate(`Area (SQM)` = as.numeric(gsub(\",\", \"\", `Area (SQM)`))) %&gt;%\n  mutate(`Unit Price ($ PSM)` = as.numeric(gsub(\"\\\\$\", \"\", gsub(\",\", \"\", `Unit Price ($ PSM)`)))) %&gt;%\n  mutate(`Area (SQM)` = ifelse(is.na(`Area (SQM)`), 0, `Area (SQM)`)) %&gt;%\n  mutate(Area_Category = cut(\n    `Area (SQM)`,\n    breaks = c(0, 100, 200, 300, 400, Inf),\n    labels = c(\"&lt;100\", \"100-200\", \"200-300\", \"300-400\", \"&gt;400\"),\n    include.lowest = TRUE\n  ))\n\ndata_cleaned &lt;- data_cleaned %&gt;%\n  mutate(Quarter = paste(year(`Sale Date`), \n                         ifelse(month(`Sale Date`) %in% c(1, 2, 3), \"Q1\",\n                                ifelse(month(`Sale Date`) %in% c(4, 5, 6), \"Q2\",\n                                       ifelse(month(`Sale Date`) %in% c(7, 8, 9), \"Q3\", \"Q4\"))), sep = \"-\"))\n\nstr(data_cleaned)\n\ntibble [26,806 × 23] (S3: tbl_df/tbl/data.frame)\n $ Project Name               : chr [1:26806] \"THE REEF AT KING'S DOCK\" \"URBAN TREASURES\" \"NORTH GAIA\" \"NORTH GAIA\" ...\n $ Transacted Price ($)       : num [1:26806] 2317000 1823500 1421112 1258112 1280000 ...\n $ Area (SQFT)                : num [1:26806] 883 883 1076 1033 872 ...\n $ Unit Price ($ PSF)         : num [1:26806] 2625 2066 1320 1218 1468 ...\n $ Sale Date                  : Date[1:26806], format: \"2023-01-01\" \"2023-01-02\" ...\n $ Address                    : chr [1:26806] \"12 HARBOURFRONT AVENUE #05-32\" \"205 JALAN EUNOS #08-02\" \"29 YISHUN CLOSE #08-10\" \"45 YISHUN CLOSE #07-42\" ...\n $ Type of Sale               : chr [1:26806] \"New Sale\" \"New Sale\" \"New Sale\" \"New Sale\" ...\n $ Type of Area               : chr [1:26806] \"Strata\" \"Strata\" \"Strata\" \"Strata\" ...\n $ Area (SQM)                 : num [1:26806] 82 82 100 96 81 ...\n $ Unit Price ($ PSM)         : num [1:26806] 28256 22238 14211 13105 15802 ...\n $ Nett Price($)              : chr [1:26806] \"-\" \"-\" \"-\" \"-\" ...\n $ Property Type              : chr [1:26806] \"Condominium\" \"Condominium\" \"Executive Condominium\" \"Executive Condominium\" ...\n $ Number of Units            : num [1:26806] 1 1 1 1 1 1 1 1 1 1 ...\n $ Tenure                     : chr [1:26806] \"99 yrs from 12/01/2021\" \"Freehold\" \"99 yrs from 15/02/2021\" \"99 yrs from 15/02/2021\" ...\n $ Completion Date            : chr [1:26806] \"Uncompleted\" \"Uncompleted\" \"Uncompleted\" \"Uncompleted\" ...\n $ Purchaser Address Indicator: chr [1:26806] \"HDB\" \"Private\" \"HDB\" \"HDB\" ...\n $ Postal Code                : chr [1:26806] \"097996\" \"419535\" \"269343\" \"269294\" ...\n $ Postal District            : chr [1:26806] \"04\" \"14\" \"27\" \"27\" ...\n $ Postal Sector              : chr [1:26806] \"09\" \"41\" \"26\" \"26\" ...\n $ Planning Region            : chr [1:26806] \"Central Region\" \"East Region\" \"North Region\" \"North Region\" ...\n $ Planning Area              : chr [1:26806] \"Bukit Merah\" \"Bedok\" \"Yishun\" \"Yishun\" ...\n $ Area_Category              : Factor w/ 5 levels \"&lt;100\",\"100-200\",..: 1 1 1 1 1 4 5 2 1 2 ...\n $ Quarter                    : chr [1:26806] \"2023-Q1\" \"2023-Q1\" \"2023-Q1\" \"2023-Q1\" ...\n\n\nValidate data according to original methods\n\nCodeResult\n\n\n\nduplicate &lt;- combined_data %&gt;% \n  group_by_all() %&gt;% \n  filter(n()&gt;1) %&gt;% \n  ungroup()\n\n\n\n\nduplicate\n\n# A tibble: 0 × 21\n# ℹ 21 variables: Project Name &lt;chr&gt;, Transacted Price ($) &lt;dbl&gt;,\n#   Area (SQFT) &lt;dbl&gt;, Unit Price ($ PSF) &lt;dbl&gt;, Sale Date &lt;chr&gt;,\n#   Address &lt;chr&gt;, Type of Sale &lt;chr&gt;, Type of Area &lt;chr&gt;, Area (SQM) &lt;dbl&gt;,\n#   Unit Price ($ PSM) &lt;dbl&gt;, Nett Price($) &lt;chr&gt;, Property Type &lt;chr&gt;,\n#   Number of Units &lt;dbl&gt;, Tenure &lt;chr&gt;, Completion Date &lt;chr&gt;,\n#   Purchaser Address Indicator &lt;chr&gt;, Postal Code &lt;chr&gt;,\n#   Postal District &lt;chr&gt;, Postal Sector &lt;chr&gt;, Planning Region &lt;chr&gt;, …\n\n\n\n\n\n\nCodePlot\n\n\n\nF1 &lt;- ggplot(combined_data, aes(x = `Property Type`)) + \n    geom_bar_interactive(aes(fill = `Planning Region`), position = \"dodge\") +  \n    labs(x = \"Property Type\", y = \"Frequency\",\n         title = \"Frequency of Property Types by Planning Region\") +\n    facet_wrap(~ `Planning Region`, scales = \"free\") +\n    theme_stata(base_size = 2.5)\n    theme(axis.text.x = element_text(angle = 45, hjust = 1)) \n\nList of 1\n $ axis.text.x:List of 11\n  ..$ family       : NULL\n  ..$ face         : NULL\n  ..$ colour       : NULL\n  ..$ size         : NULL\n  ..$ hjust        : num 1\n  ..$ vjust        : NULL\n  ..$ angle        : num 45\n  ..$ lineheight   : NULL\n  ..$ margin       : NULL\n  ..$ debug        : NULL\n  ..$ inherit.blank: logi FALSE\n  ..- attr(*, \"class\")= chr [1:2] \"element_text\" \"element\"\n - attr(*, \"class\")= chr [1:2] \"theme\" \"gg\"\n - attr(*, \"complete\")= logi FALSE\n - attr(*, \"validate\")= logi TRUE\n\n\n\n\n\n    F1"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#dataviz-makeover",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#dataviz-makeover",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "The absence of data from the previous year’s four quarters in the scatter plot analyzing the relationship between transacted prices and area poses certain challenges. Primarily, it limits the depth of market trend analysis over an extended period, potentially overlooking seasonal fluctuations and economic cycles that significantly influence real estate markets. Without this broader temporal context, the analysis might give undue emphasis to short-term variations or isolated events, potentially leading to a somewhat narrow perspective on market dynamics. Such a limitation curtails the capacity to observe and interpret long-term trends and the cyclical nature of the housing market, which are essential for comprehensive statistical analysis and forecasting.\nFurthermore, the lack of previous year data restricts year-over-year comparative analysis, crucial for evaluating market condition changes and understanding the influence of external economic factors like interest rate shifts or broader economic changes. For investors and policymakers, the unavailability of a more extended data range may hinder informed strategic decision-making. It limits the development of predictive models that rely on historical data to accurately project future market behaviors, thereby subtly reducing the analysis’s overall utility in supporting nuanced business and investment decisions.\nTo address the challenges of a limited temporal analysis, I have enhanced the dataset by incorporating data from the previous year and categorizing it by quarters. This addition allows for a more nuanced exploration of the changes and trends over time. By analyzing the data on a quarterly basis, we can better understand the seasonal fluctuations and the impact of economic cycles on the real estate market.\n\nCodePlotConclusion\n\n\n\nlibrary(plotly)\n\nP3_interactive &lt;- plot_ly(data = data_cleaned, x = ~`Area (SQFT)`, y = ~`Transacted Price ($)`, type = 'scatter', mode = 'markers',\n                          hoverinfo = 'text',\n                          transforms = list(\n                            list(\n                              type = 'filter',\n                              target = ~Quarter,\n                              operation = '=',\n                              value = unique(data_cleaned$Quarter)[1]  \n                            )\n                          ),\n                          text = ~paste(\"Price: \", `Transacted Price ($)`, \"&lt;br&gt;Area: \", `Area (SQFT)`, \"&lt;br&gt;Quarter: \", Quarter)) %&gt;%\n  layout(title = 'Scatter Plot of Transacted Price vs. Area',\n         xaxis = list(title = 'Area (SQFT)'),\n         yaxis = list(title = 'Transacted Price ($)'),\n         sliders = list(list(\n           active = 0,\n           currentvalue = list(prefix = \"Quarter: \"),\n           steps = lapply(unique(data_cleaned$Quarter), function(q) {\n             list(label = q, method = \"restyle\", args = list(\"transforms[0].value\", q))\n           })\n         )))\n\n\n\n\nP3_interactive\n\n\n\n\n\n\n\nThe interactive scatter plot offers several advantages over the static scatter plot with a trend line, particularly in terms of user engagement and analytical depth. Firstly, its interactivity enhances user experience by allowing for the exploration of individual data points through tooltips and dynamic elements such as sliders for temporal data filtering. This interactivity not only makes the data more accessible but also enables users to perform on-the-fly analysis of specific time periods without the need to switch between different plots. Moreover, the ability to zoom in and navigate through dense clusters of data points helps in examining detailed patterns that might be obscured in a static plot. This dynamic functionality is especially valuable in digital platforms where users expect an interactive and engaging data visualization experience.\n\n\n\n\n\n\nFailing to categorize data by “Type of Sale” in an analysis of real estate transactions introduces significant limitations that can obscure crucial insights into different market segments. Without this categorization, the analysis merges various types of transactions, such as new sales, resales, and sub-sales, into a single aggregated dataset. This aggregation can lead to a generalized overview that fails to capture the distinct behaviors and trends associated with each sale type.\nFirstly, different sale types often exhibit unique pricing patterns, demand cycles, and buyer preferences. For instance, new sales might be influenced by developer promotions and economic incentives, while resales are impacted more by the existing housing market conditions. Sub-sales, involving properties sold before their construction completion, might fluctuate based on speculative market sentiments. Without distinguishing these types, strategic decision-making becomes challenging as the nuanced dynamics of the market are not adequately represented.\nMoreover, policies and marketing strategies tailored to specific sale types cannot be effectively formulated or implemented without a clear understanding of the particular characteristics and needs of each segment. For example, marketing strategies that are effective for new developments might not work for resales. Therefore, by not categorizing data by “Type of Sale,” the analysis loses the potential to guide targeted interventions and optimize resource allocation, potentially leading to less effective strategies and missed opportunities in the market.\nTo enhance our analysis, I’ve processed the data to categorize it by different “Type of Sale.” This categorization allows us to delve into the specific characteristics and trends of new sales, resales, and sub-sales separately.\n\nCodePlotConclusion\n\n\n\nlibrary(plotly)\n\nP3_interactive &lt;- plot_ly(data = data_cleaned, x = ~`Area (SQFT)`, y = ~`Transacted Price ($)`, type = 'scatter', mode = 'markers',\n                          hoverinfo = 'text',\n                          transforms = list(\n                            list(\n                              type = 'filter',\n                              target = ~Quarter,\n                              operation = '=',\n                              value = unique(data_cleaned$Quarter)[1] \n                            ),\n                            list(\n                              type = 'filter',\n                              target = ~`Type of Sale`,\n                              operation = '=',\n                              value = unique(data_cleaned$`Type of Sale`)[1] \n                            )\n                          ),\n                          text = ~paste(\"Price: \", `Transacted Price ($)`, \"&lt;br&gt;Area: \", `Area (SQFT)`, \"&lt;br&gt;Quarter: \", Quarter, \"&lt;br&gt;Type of Sale: \", `Type of Sale`)) %&gt;%\n  layout(title = 'Scatter Plot of Transacted Price vs. Area',\n         xaxis = list(title = 'Area (SQFT)'),\n         yaxis = list(title = 'Transacted Price ($)'),\n         updatemenus = list(\n           list(\n             type = \"dropdown\",\n             direction = \"down\",\n             showactive = TRUE,\n             buttons = lapply(unique(data_cleaned$`Type of Sale`), function(type) {\n               list(\n                 method = \"restyle\",\n                 args = list(\"transforms[1].value\", type),\n                 label = type\n               )\n             })\n           )\n         ),\n         sliders = list(list(\n           active = 0,\n           currentvalue = list(prefix = \"Quarter: \"),\n           steps = lapply(unique(data_cleaned$Quarter), function(q) {\n             list(\n               label = q,\n               method = \"restyle\",\n               args = list(\"transforms[0].value\", q)  \n             )\n           })\n         )))\n\n\n\n\nP3_interactive\n\n\n\n\n\n\n\nThe updated interactive scatter plot enhances data analysis capabilities significantly by incorporating dropdown menus for category-based filtering, such as transaction types (New Sale, Resale, Sub Sale). This feature allows users to segment the dataset on the fly, enabling a focused examination of trends and patterns specific to each category. Such granularity is invaluable for users needing to make nuanced assessments of market dynamics. Furthermore, the plot’s clear presentation even in high-density areas ensures that all data points are easily visible and distinguishable, avoiding any visual confusion and facilitating precise analysis. Enhanced interactivity, provided by the dropdown filters, improves the user experience by allowing dynamic and complex data segmentation within the visualization interface itself. This makes the plot a highly effective tool for stakeholders requiring a comprehensive and detailed view of various market conditions without the need for additional tools or programming efforts."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#reference",
    "href": "Take-home_Ex/Take-home_Ex02/Take-home_Ex02.html#reference",
    "title": "Take-home_Ex02",
    "section": "",
    "text": "URA releases flash estimate of 1st Quarter 2024 private residential property price index\nUnsold private housing stock on the rise ahead of ramp-up in new launches in 2024\nHDB resale prices rise 1.7%; private home prices up 1.5% in first quarter: Flash estimates"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Easter Eggs",
    "section": "",
    "text": "There are no\n\n\nEaster Eggs up here.\n\n\nGo away."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "There are two major residential property market in Singapore, namely public and private housing. Public housing aims to meet the basic need of the general public with monthly household income less than or equal to S$14,000. For families with monthly household income more than S$14,000, they need to turn to the private residential market.\n\n\n\nTo accomplish the task, transaction data of REALIS will be used. A complete set of the private residential property transaction data from 1st January 2023 to 31st March 2024 have been downloaded via SMU e-library database service.\n\n\n\nAssuming the role of a graphical editor of a median company, we prepare minimum two and maximum three data visualisation to reveal the private residential market and sub-markets of Singapore for the 1st quarter of 2024.\n\n\n\n\n\n\ntidyverse (i.e. readr, tidyr, dplyr, ggplot2) :for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nlubridate: Simplifies the handling of dates and times in R by providing tools to parse, manipulate, and calculate with date-time objects more intuitively.\npatchwork: An extension for ggplot2 that enables the easy assembly of multiple plots to produce a composite plot that can reveal complex insights into data.\nzoo: Provides powerful methods for managing and manipulating ordered indexed data, particularly time series data, making it easier to handle a variety of data irregularities.\n\npacman::p_load(tidyverse, lubridate, patchwork, zoo)\n\n\n\n\n\n\n\n\n\n\n\nCategory\nDescription\n\n\n\n\nProject Name\nthe name of the property or title of the development project that is being sold\n\n\nSale Date\nThe specific date when the transaction was completed\n\n\nType of Sale\nA new sale, resale, or auction, among other types.Here mainly analyze resale\n\n\nArea (SQM)\nThe usable floor area of the property in square meters\n\n\nUnit Price ($ PSM)\nThe price per square meter of the property\n\n\n\n\n\n\n\n\n\nEach read_csv() function loads data from a CSV file into R, creating five separate data frames: data1, data2, data3, data4, and data5. 2.Data Consolidation:\nbind_rows() from the dplyr package merges these five data frames into one large data frame called data. This function stacks the data frames vertically, allowing for an analysis across all data combined. 3.Data Inspection:\nglimpse() provides a quick overview of the combined data frame, showing types of data and a preview of the first few entries in each column, helping to quickly assess the data’s structure and readiness for analysis.\n\ndata1 = read_csv(\"data/ResidentialTransaction20240308160536.csv\")\ndata2 = read_csv(\"data/ResidentialTransaction20240308160736.csv\")\ndata3 = read_csv(\"data/ResidentialTransaction20240308161009.csv\")\ndata4 = read_csv(\"data/ResidentialTransaction20240308161109.csv\")\ndata5 = read_csv(\"data/ResidentialTransaction20240414220633.csv\")\ndata &lt;- bind_rows(data1, data2, data3, data4, data5)\n\nglimpse(data)\n\nRows: 26,806\nColumns: 21\n$ `Project Name`                &lt;chr&gt; \"THE REEF AT KING'S DOCK\", \"URBAN TREASU…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2317000, 1823500, 1421112, 1258112, 1280…\n$ `Area (SQFT)`                 &lt;dbl&gt; 882.65, 882.65, 1076.40, 1033.34, 871.88…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2625, 2066, 1320, 1218, 1468, 1767, 1095…\n$ `Sale Date`                   &lt;chr&gt; \"01 Jan 2023\", \"02 Jan 2023\", \"02 Jan 20…\n$ Address                       &lt;chr&gt; \"12 HARBOURFRONT AVENUE #05-32\", \"205 JA…\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Strata\", \"Strata\", \"Strata\", …\n$ `Area (SQM)`                  &lt;dbl&gt; 82.0, 82.0, 100.0, 96.0, 81.0, 308.7, 42…\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 28256, 22238, 14211, 13105, 15802, 19015…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Condominium\", \"Executive…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"99 yrs from 12/01/2021\", \"Freehold\", \"9…\n$ `Completion Date`             &lt;chr&gt; \"Uncompleted\", \"Uncompleted\", \"Uncomplet…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"HDB\", \"Private\", \"HDB\", \"HDB\", \"HDB\", \"…\n$ `Postal Code`                 &lt;chr&gt; \"097996\", \"419535\", \"269343\", \"269294\", …\n$ `Postal District`             &lt;chr&gt; \"04\", \"14\", \"27\", \"27\", \"28\", \"19\", \"10\"…\n$ `Postal Sector`               &lt;chr&gt; \"09\", \"41\", \"26\", \"26\", \"79\", \"54\", \"27\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"East Region\", \"North …\n$ `Planning Area`               &lt;chr&gt; \"Bukit Merah\", \"Bedok\", \"Yishun\", \"Yishu…\n\n\n\n\n\nSale Date is converted to a date format using the dmy() function from the lubridate package, which interprets strings as dates in “day-month-year” format.\nArea (SQM). is transformed into a numeric variable. First, any commas are removed with gsub(“,”, ““, Area (SQM).), then the result is coerced to numeric with as.numeric().\nUnit Price ($ PSM). undergoes a similar process as Area (SQM).; however, it also removes the dollar sign ($) before removing commas and converting to numeric.\nAny NA (missing) values in Area (SQM). are replaced with 0 using ifelse(is.na(Area (SQM).), 0, Area (SQM).).\nA new variable Area_Category is created using the cut() function to categorize the Area (SQM). variable into predefined bins: &lt;100, 100-200, 200-300, 300-400, &gt;400.\nThe breaks argument specifies the boundaries for these bins, and labels provides the corresponding category labels.\ninclude.lowest = TRUE ensures that values equal to the lowest break point (0) are included in the first category (&lt;100).\n\ndata_cleaned &lt;- data %&gt;%\n  mutate(\n    `Sale Date` = dmy(`Sale Date`),  # Ensuring we're using the exact column name from the dataset\n    `Area (SQM)` = as.numeric(gsub(\",\", \"\", `Area (SQM)`)),  # Keeping the variable names as they are in the dataset\n    `Unit Price ($ PSM)` = as.numeric(gsub(\"\\\\$\", \"\", gsub(\",\", \"\", `Unit Price ($ PSM)`)))  # Properly referencing the variable\n  )\n\ndata_cleaned &lt;- data_cleaned %&gt;%\n  mutate(\n    `Area (SQM)` = ifelse(is.na(`Area (SQM)`), 0, `Area (SQM)`)  # Dealing with NA values\n  )\n\ndata_cleaned &lt;- data_cleaned %&gt;%\n  mutate(\n    Area_Category = cut(\n      `Area (SQM)`,\n      breaks = c(0, 100, 200, 300, 400, Inf),  \n      labels = c(\"&lt;100\", \"100-200\", \"200-300\", \"300-400\", \"&gt;400\"),\n      include.lowest = TRUE  \n    )\n  )\n\n\n\n\n\nThe visualizations specifically highlight resale data within Singapore’s private residential market for the first quarter of 2024. This focus on resale properties is crucial as it offers a snapshot of the market’s secondary transactions, reflecting ongoing demand and providing a real-world gauge of property values after the initial purchase. Resale data represents an essential segment of the housing market, often characterized by immediate habitability and established neighborhoods, crucial factors for many homebuyers and investors in making informed decisions.\n\n\nSale Date:\nThe Sale Date variable is crucial in housing price analysis for monitoring temporal patterns. It allows analysts to track how prices change over time, revealing trends that may correspond to economic conditions, interest rates, or seasonal fluctuations. By examining the Sale Date, one can also discern if there are particular times of the year when the market peaks or dips, which can be invaluable for both buyers looking for the best deal and sellers aiming for the optimal listing period.\nArea and Unit Price:\nOn the other hand, Area (SQM). and Unit Price ($ PSM). provide spatial and valuation metrics, respectively. The area of a property, often measured in square meters, directly influences its utility and potential for future modifications, affecting its market value. The unit price per square meter standardizes property values, making it possible to compare homes of different sizes and locations fairly. This metric is essential to understand what drives value in the housing market and to identify whether buyers are getting a reasonable price for the area they are purchasing in.\n\nCodePlotConclusion\n\n\n\nresale_data &lt;- data_cleaned %&gt;%\n  filter(`Type of Sale` == \"Resale\") %&gt;%\n  mutate(Quarter = as.yearqtr(`Sale Date`))\n\navg_price_by_quarter_resale &lt;- resale_data %&gt;%\n  group_by(Area_Category, Quarter) %&gt;%\n  summarise(Avg_Unit_Price_PSM = mean(`Unit Price ($ PSM)`, na.rm = TRUE), .groups = \"drop\")\n\np_line_resale &lt;- avg_price_by_quarter_resale %&gt;%\n  ggplot(aes(x = Quarter, y = Avg_Unit_Price_PSM, group = Area_Category, color = Area_Category)) +\n  geom_line() +\n  labs(title = \"Quarterly Average Unit Price ($ PSM) by Area Category for Resale\",\n       x = \"Quarter\",\n       y = \"Average Price ($ PSM)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\np_line_resale_stacked &lt;- p_line_resale + facet_wrap(~Area_Category, ncol = 1, scales = \"free_y\")\n\ndata_q1_resale &lt;- resale_data %&gt;%\n  filter(Quarter == \"2024 Q1\") %&gt;%\n  mutate(Month = floor_date(`Sale Date`, \"month\"))\n\np_box_resale &lt;- ggplot(data_q1_resale, aes(x = Area_Category, y = `Unit Price ($ PSM)`, fill = Area_Category)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, alpha = 0.001) +  # Adjusting alpha for visibility\n  facet_wrap(~Month, scales = \"free_y\") +\n  labs(title = \"Monthly Unit Price ($ PSM) by Area Category for Resale (Q1 2024)\",\n       x = \"Area Category\",\n       y = \"Price ($ PSM)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\np_combined_resale &lt;- p_line_resale_stacked / p_box_resale\n\n\n\n\nprint(p_combined_resale)\n\n\n\n\n\n\nIn the resale housing market, the quarterly average unit price per square meter (PSM) exhibits contrasting trends: smaller properties (&lt;200 SQM) are gaining value, while larger ones (≥300 SQM) show price declines, indicating a shift in consumer preferences toward more compact living spaces. The Q1 2024 box plots reveal a broad price range within each area category, especially in the largest homes, suggesting that factors like location and amenities significantly influence pricing.\n\n\n\n\n\n\nProject Name:\nThe variable Project.Name represents the specific housing development or project. Analyzing data by Project.Name allows for a detailed understanding of the popularity and demand for specific developments. This can highlight developers’ reputation, location desirability, and unique features of individual projects that may influence prices. Trends can emerge, such as certain developers commanding premium prices or projects in specific locations being more sought-after.\nTotal Units and Avg Unit Price PSM:\nTotal_Units and Avg_Unit_Price_PSM are key metrics for assessing supply and value. Total_Units sold provides insight into the scale of a project and its market absorption rate. High sales volumes can indicate high demand or effective pricing strategies, while low volumes may signal overpricing or less desirable attributes. Avg_Unit_Price_PSM (average unit price per square meter) offers a comparative value indicator that normalizes prices across different sizes and types of properties, reflecting the price point at which the market clears. Analyzing this alongside Total_Units can reveal if there’s a relationship between the number of units sold and the price points, potentially guiding future development and pricing strategies.\n\nCodePlotConclusion\n\n\n\nresale_data &lt;- data_cleaned %&gt;%\n  filter(`Type of Sale` == \"Resale\")\n\nproject_stats &lt;- resale_data %&gt;%\n  filter(`Project Name` != \"N.A.\") %&gt;%\n  group_by(`Project Name`) %&gt;%\n  summarise(\n    Total_Units = sum(`Number of Units`, na.rm = TRUE),  # Sum up all units for each project\n    Avg_Unit_Price = mean(`Unit Price ($ PSM)`, na.rm = TRUE),  # Calculate the average price per square meter\n    .groups = \"drop\"  # Drop the grouping\n  )\n\ntop_projects &lt;- project_stats %&gt;%\n  top_n(25, Total_Units) %&gt;%\n  arrange(desc(Total_Units))\n\nproject_order &lt;- top_projects$`Project Name`\n\ntop_projects$`Project Name` &lt;- factor(top_projects$`Project Name`, levels = project_order)\nresale_data$`Project Name` &lt;- factor(resale_data$`Project Name`, levels = project_order)\n\np_units &lt;- ggplot(top_projects, aes(x = `Project Name`, y = Total_Units, fill = 'steelblue')) +\n  geom_bar(stat = \"identity\") +\n  labs(y = \"Total Number of Units\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),\n    legend.position = \"none\"\n  )\n\np_price &lt;- ggplot(resale_data, aes(x = `Project Name`, y = `Unit Price ($ PSM)`)) +\n  geom_boxplot() +\n  labs(y = \"Unit Price ($ PSM)\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),\n    legend.position = \"none\"\n  )\n\np_combined &lt;- p_units | p_price\n\n\n\n\np_combined\n\n\n\n\n\n\nThe bar chart depicts a significant lead in the total number of units sold by “Sol Acres,” suggesting a high demand or larger inventory. Other projects show a steady decrease in volume, indicating varied market preferences. The box plot showcases consistent unit price ranges within most projects, with notable premiums in “The Sail @ Marina Bay” and “Reflections at Keppel Bay,” likely due to their desirable locations or upscale amenities.\n\n\n\n\n\n\n\nThe data from the first quarter of 2024 shows that “Sol Acres” significantly leads in resale volumes, hinting at its market desirability, possibly due to a balance of affordability and attractive amenities. This project’s success may be a key indicator for those seeking value-for-money investments. In contrast, luxury segments like “The Sail @ Marina Bay” exhibit a wide price range, indicating a market for buyers with a taste for exclusivity and willingness to pay a premium for distinctive features.\nFor Consumers: Buyers should align their choices with their priorities: affordability may lead them to high-volume projects, while unique, high-quality features could draw them towards premium segments. When considering investment, a diversified approach that includes stable, high-volume properties and selective, high-value opportunities may offer balanced returns.\n\n\n\n\nURA releases flash estimate of 1st Quarter 2024 private residential property price index\nUnsold private housing stock on the rise ahead of ramp-up in new launches in 2024\nHDB resale prices rise 1.7%; private home prices up 1.5% in first quarter: Flash estimates"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#overview",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#overview",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "There are two major residential property market in Singapore, namely public and private housing. Public housing aims to meet the basic need of the general public with monthly household income less than or equal to S$14,000. For families with monthly household income more than S$14,000, they need to turn to the private residential market.\n\n\n\nTo accomplish the task, transaction data of REALIS will be used. A complete set of the private residential property transaction data from 1st January 2023 to 31st March 2024 have been downloaded via SMU e-library database service.\n\n\n\nAssuming the role of a graphical editor of a median company, we prepare minimum two and maximum three data visualisation to reveal the private residential market and sub-markets of Singapore for the 1st quarter of 2024."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#getting-start",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#getting-start",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "tidyverse (i.e. readr, tidyr, dplyr, ggplot2) :for performing data science tasks such as importing, tidying, and wrangling data, as well as creating graphics based on The Grammar of Graphics;\nlubridate: Simplifies the handling of dates and times in R by providing tools to parse, manipulate, and calculate with date-time objects more intuitively.\npatchwork: An extension for ggplot2 that enables the easy assembly of multiple plots to produce a composite plot that can reveal complex insights into data.\nzoo: Provides powerful methods for managing and manipulating ordered indexed data, particularly time series data, making it easier to handle a variety of data irregularities.\n\npacman::p_load(tidyverse, lubridate, patchwork, zoo)\n\n\n\n\n\n\n\n\n\n\n\nCategory\nDescription\n\n\n\n\nProject Name\nthe name of the property or title of the development project that is being sold\n\n\nSale Date\nThe specific date when the transaction was completed\n\n\nType of Sale\nA new sale, resale, or auction, among other types.Here mainly analyze resale\n\n\nArea (SQM)\nThe usable floor area of the property in square meters\n\n\nUnit Price ($ PSM)\nThe price per square meter of the property"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#data-processing",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#data-processing",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "Each read_csv() function loads data from a CSV file into R, creating five separate data frames: data1, data2, data3, data4, and data5. 2.Data Consolidation:\nbind_rows() from the dplyr package merges these five data frames into one large data frame called data. This function stacks the data frames vertically, allowing for an analysis across all data combined. 3.Data Inspection:\nglimpse() provides a quick overview of the combined data frame, showing types of data and a preview of the first few entries in each column, helping to quickly assess the data’s structure and readiness for analysis.\n\ndata1 = read_csv(\"data/ResidentialTransaction20240308160536.csv\")\ndata2 = read_csv(\"data/ResidentialTransaction20240308160736.csv\")\ndata3 = read_csv(\"data/ResidentialTransaction20240308161009.csv\")\ndata4 = read_csv(\"data/ResidentialTransaction20240308161109.csv\")\ndata5 = read_csv(\"data/ResidentialTransaction20240414220633.csv\")\ndata &lt;- bind_rows(data1, data2, data3, data4, data5)\n\nglimpse(data)\n\nRows: 26,806\nColumns: 21\n$ `Project Name`                &lt;chr&gt; \"THE REEF AT KING'S DOCK\", \"URBAN TREASU…\n$ `Transacted Price ($)`        &lt;dbl&gt; 2317000, 1823500, 1421112, 1258112, 1280…\n$ `Area (SQFT)`                 &lt;dbl&gt; 882.65, 882.65, 1076.40, 1033.34, 871.88…\n$ `Unit Price ($ PSF)`          &lt;dbl&gt; 2625, 2066, 1320, 1218, 1468, 1767, 1095…\n$ `Sale Date`                   &lt;chr&gt; \"01 Jan 2023\", \"02 Jan 2023\", \"02 Jan 20…\n$ Address                       &lt;chr&gt; \"12 HARBOURFRONT AVENUE #05-32\", \"205 JA…\n$ `Type of Sale`                &lt;chr&gt; \"New Sale\", \"New Sale\", \"New Sale\", \"New…\n$ `Type of Area`                &lt;chr&gt; \"Strata\", \"Strata\", \"Strata\", \"Strata\", …\n$ `Area (SQM)`                  &lt;dbl&gt; 82.0, 82.0, 100.0, 96.0, 81.0, 308.7, 42…\n$ `Unit Price ($ PSM)`          &lt;dbl&gt; 28256, 22238, 14211, 13105, 15802, 19015…\n$ `Nett Price($)`               &lt;chr&gt; \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", …\n$ `Property Type`               &lt;chr&gt; \"Condominium\", \"Condominium\", \"Executive…\n$ `Number of Units`             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ Tenure                        &lt;chr&gt; \"99 yrs from 12/01/2021\", \"Freehold\", \"9…\n$ `Completion Date`             &lt;chr&gt; \"Uncompleted\", \"Uncompleted\", \"Uncomplet…\n$ `Purchaser Address Indicator` &lt;chr&gt; \"HDB\", \"Private\", \"HDB\", \"HDB\", \"HDB\", \"…\n$ `Postal Code`                 &lt;chr&gt; \"097996\", \"419535\", \"269343\", \"269294\", …\n$ `Postal District`             &lt;chr&gt; \"04\", \"14\", \"27\", \"27\", \"28\", \"19\", \"10\"…\n$ `Postal Sector`               &lt;chr&gt; \"09\", \"41\", \"26\", \"26\", \"79\", \"54\", \"27\"…\n$ `Planning Region`             &lt;chr&gt; \"Central Region\", \"East Region\", \"North …\n$ `Planning Area`               &lt;chr&gt; \"Bukit Merah\", \"Bedok\", \"Yishun\", \"Yishu…\n\n\n\n\n\nSale Date is converted to a date format using the dmy() function from the lubridate package, which interprets strings as dates in “day-month-year” format.\nArea (SQM). is transformed into a numeric variable. First, any commas are removed with gsub(“,”, ““, Area (SQM).), then the result is coerced to numeric with as.numeric().\nUnit Price ($ PSM). undergoes a similar process as Area (SQM).; however, it also removes the dollar sign ($) before removing commas and converting to numeric.\nAny NA (missing) values in Area (SQM). are replaced with 0 using ifelse(is.na(Area (SQM).), 0, Area (SQM).).\nA new variable Area_Category is created using the cut() function to categorize the Area (SQM). variable into predefined bins: &lt;100, 100-200, 200-300, 300-400, &gt;400.\nThe breaks argument specifies the boundaries for these bins, and labels provides the corresponding category labels.\ninclude.lowest = TRUE ensures that values equal to the lowest break point (0) are included in the first category (&lt;100).\n\ndata_cleaned &lt;- data %&gt;%\n  mutate(\n    `Sale Date` = dmy(`Sale Date`),  # Ensuring we're using the exact column name from the dataset\n    `Area (SQM)` = as.numeric(gsub(\",\", \"\", `Area (SQM)`)),  # Keeping the variable names as they are in the dataset\n    `Unit Price ($ PSM)` = as.numeric(gsub(\"\\\\$\", \"\", gsub(\",\", \"\", `Unit Price ($ PSM)`)))  # Properly referencing the variable\n  )\n\ndata_cleaned &lt;- data_cleaned %&gt;%\n  mutate(\n    `Area (SQM)` = ifelse(is.na(`Area (SQM)`), 0, `Area (SQM)`)  # Dealing with NA values\n  )\n\ndata_cleaned &lt;- data_cleaned %&gt;%\n  mutate(\n    Area_Category = cut(\n      `Area (SQM)`,\n      breaks = c(0, 100, 200, 300, 400, Inf),  \n      labels = c(\"&lt;100\", \"100-200\", \"200-300\", \"300-400\", \"&gt;400\"),\n      include.lowest = TRUE  \n    )\n  )"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#exploratory-data-analysis",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#exploratory-data-analysis",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The visualizations specifically highlight resale data within Singapore’s private residential market for the first quarter of 2024. This focus on resale properties is crucial as it offers a snapshot of the market’s secondary transactions, reflecting ongoing demand and providing a real-world gauge of property values after the initial purchase. Resale data represents an essential segment of the housing market, often characterized by immediate habitability and established neighborhoods, crucial factors for many homebuyers and investors in making informed decisions.\n\n\nSale Date:\nThe Sale Date variable is crucial in housing price analysis for monitoring temporal patterns. It allows analysts to track how prices change over time, revealing trends that may correspond to economic conditions, interest rates, or seasonal fluctuations. By examining the Sale Date, one can also discern if there are particular times of the year when the market peaks or dips, which can be invaluable for both buyers looking for the best deal and sellers aiming for the optimal listing period.\nArea and Unit Price:\nOn the other hand, Area (SQM). and Unit Price ($ PSM). provide spatial and valuation metrics, respectively. The area of a property, often measured in square meters, directly influences its utility and potential for future modifications, affecting its market value. The unit price per square meter standardizes property values, making it possible to compare homes of different sizes and locations fairly. This metric is essential to understand what drives value in the housing market and to identify whether buyers are getting a reasonable price for the area they are purchasing in.\n\nCodePlotConclusion\n\n\n\nresale_data &lt;- data_cleaned %&gt;%\n  filter(`Type of Sale` == \"Resale\") %&gt;%\n  mutate(Quarter = as.yearqtr(`Sale Date`))\n\navg_price_by_quarter_resale &lt;- resale_data %&gt;%\n  group_by(Area_Category, Quarter) %&gt;%\n  summarise(Avg_Unit_Price_PSM = mean(`Unit Price ($ PSM)`, na.rm = TRUE), .groups = \"drop\")\n\np_line_resale &lt;- avg_price_by_quarter_resale %&gt;%\n  ggplot(aes(x = Quarter, y = Avg_Unit_Price_PSM, group = Area_Category, color = Area_Category)) +\n  geom_line() +\n  labs(title = \"Quarterly Average Unit Price ($ PSM) by Area Category for Resale\",\n       x = \"Quarter\",\n       y = \"Average Price ($ PSM)\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"none\")\n\np_line_resale_stacked &lt;- p_line_resale + facet_wrap(~Area_Category, ncol = 1, scales = \"free_y\")\n\ndata_q1_resale &lt;- resale_data %&gt;%\n  filter(Quarter == \"2024 Q1\") %&gt;%\n  mutate(Month = floor_date(`Sale Date`, \"month\"))\n\np_box_resale &lt;- ggplot(data_q1_resale, aes(x = Area_Category, y = `Unit Price ($ PSM)`, fill = Area_Category)) +\n  geom_boxplot() +\n  geom_jitter(width = 0.2, alpha = 0.001) +  # Adjusting alpha for visibility\n  facet_wrap(~Month, scales = \"free_y\") +\n  labs(title = \"Monthly Unit Price ($ PSM) by Area Category for Resale (Q1 2024)\",\n       x = \"Area Category\",\n       y = \"Price ($ PSM)\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\",\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\np_combined_resale &lt;- p_line_resale_stacked / p_box_resale\n\n\n\n\nprint(p_combined_resale)\n\n\n\n\n\n\nIn the resale housing market, the quarterly average unit price per square meter (PSM) exhibits contrasting trends: smaller properties (&lt;200 SQM) are gaining value, while larger ones (≥300 SQM) show price declines, indicating a shift in consumer preferences toward more compact living spaces. The Q1 2024 box plots reveal a broad price range within each area category, especially in the largest homes, suggesting that factors like location and amenities significantly influence pricing.\n\n\n\n\n\n\nProject Name:\nThe variable Project.Name represents the specific housing development or project. Analyzing data by Project.Name allows for a detailed understanding of the popularity and demand for specific developments. This can highlight developers’ reputation, location desirability, and unique features of individual projects that may influence prices. Trends can emerge, such as certain developers commanding premium prices or projects in specific locations being more sought-after.\nTotal Units and Avg Unit Price PSM:\nTotal_Units and Avg_Unit_Price_PSM are key metrics for assessing supply and value. Total_Units sold provides insight into the scale of a project and its market absorption rate. High sales volumes can indicate high demand or effective pricing strategies, while low volumes may signal overpricing or less desirable attributes. Avg_Unit_Price_PSM (average unit price per square meter) offers a comparative value indicator that normalizes prices across different sizes and types of properties, reflecting the price point at which the market clears. Analyzing this alongside Total_Units can reveal if there’s a relationship between the number of units sold and the price points, potentially guiding future development and pricing strategies.\n\nCodePlotConclusion\n\n\n\nresale_data &lt;- data_cleaned %&gt;%\n  filter(`Type of Sale` == \"Resale\")\n\nproject_stats &lt;- resale_data %&gt;%\n  filter(`Project Name` != \"N.A.\") %&gt;%\n  group_by(`Project Name`) %&gt;%\n  summarise(\n    Total_Units = sum(`Number of Units`, na.rm = TRUE),  # Sum up all units for each project\n    Avg_Unit_Price = mean(`Unit Price ($ PSM)`, na.rm = TRUE),  # Calculate the average price per square meter\n    .groups = \"drop\"  # Drop the grouping\n  )\n\ntop_projects &lt;- project_stats %&gt;%\n  top_n(25, Total_Units) %&gt;%\n  arrange(desc(Total_Units))\n\nproject_order &lt;- top_projects$`Project Name`\n\ntop_projects$`Project Name` &lt;- factor(top_projects$`Project Name`, levels = project_order)\nresale_data$`Project Name` &lt;- factor(resale_data$`Project Name`, levels = project_order)\n\np_units &lt;- ggplot(top_projects, aes(x = `Project Name`, y = Total_Units, fill = 'steelblue')) +\n  geom_bar(stat = \"identity\") +\n  labs(y = \"Total Number of Units\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),\n    legend.position = \"none\"\n  )\n\np_price &lt;- ggplot(resale_data, aes(x = `Project Name`, y = `Unit Price ($ PSM)`)) +\n  geom_boxplot() +\n  labs(y = \"Unit Price ($ PSM)\") +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),\n    legend.position = \"none\"\n  )\n\np_combined &lt;- p_units | p_price\n\n\n\n\np_combined\n\n\n\n\n\n\nThe bar chart depicts a significant lead in the total number of units sold by “Sol Acres,” suggesting a high demand or larger inventory. Other projects show a steady decrease in volume, indicating varied market preferences. The box plot showcases consistent unit price ranges within most projects, with notable premiums in “The Sail @ Marina Bay” and “Reflections at Keppel Bay,” likely due to their desirable locations or upscale amenities."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#conclusion-2",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#conclusion-2",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "The data from the first quarter of 2024 shows that “Sol Acres” significantly leads in resale volumes, hinting at its market desirability, possibly due to a balance of affordability and attractive amenities. This project’s success may be a key indicator for those seeking value-for-money investments. In contrast, luxury segments like “The Sail @ Marina Bay” exhibit a wide price range, indicating a market for buyers with a taste for exclusivity and willingness to pay a premium for distinctive features.\nFor Consumers: Buyers should align their choices with their priorities: affordability may lead them to high-volume projects, while unique, high-quality features could draw them towards premium segments. When considering investment, a diversified approach that includes stable, high-volume properties and selective, high-value opportunities may offer balanced returns."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#reference",
    "href": "Take-home_Ex/Take-home_Ex01/Take-home Exercise 1.html#reference",
    "title": "Take-home Exercise 1",
    "section": "",
    "text": "URA releases flash estimate of 1st Quarter 2024 private residential property price index\nUnsold private housing stock on the rise ahead of ramp-up in new launches in 2024\nHDB resale prices rise 1.7%; private home prices up 1.5% in first quarter: Flash estimates"
  },
  {
    "objectID": "In-class_EX/In-class_EX05/In-class_EX05.html",
    "href": "In-class_EX/In-class_EX05/In-class_EX05.html",
    "title": "In-class_Ex05",
    "section": "",
    "text": "Put all data into one tibular dataframe.\n\npacman::p_load(tidyverse, readtext,\n               quanteda, tidytext)\n\n\ndata_folder &lt;- \"MC1/articles\"\n\nText sensing to extract text\n\ntext_data &lt;- readtext(paste0(\"MC1/articles\",\n                \"/*\"))\n\nOR\n\ntext_data &lt;- readtext(\"MC1/articles\")\n\nBasic tokenisation\n\nusenet_words &lt;- text_data %&gt;%\n  unnest_tokens(word, text) %&gt;%  #reading the text data\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word) #remove stop words\n\n\nusenet_words %&gt;%\n  count(word, sort = TRUE)\n\nreadtext object consisting of 3260 documents and 0 docvars.\n# A data frame: 3,260 × 3\n  word             n text     \n  &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;    \n1 fishing       2177 \"\\\"\\\"...\"\n2 sustainable   1525 \"\\\"\\\"...\"\n3 company       1036 \"\\\"\\\"...\"\n4 practices      838 \"\\\"\\\"...\"\n5 industry       715 \"\\\"\\\"...\"\n6 transactions   696 \"\\\"\\\"...\"\n# ℹ 3,254 more rows\n\n\nObservations- Most common words are: fishing, sustainable, company\n\ntemp_table &lt;- usenet_words %&gt;%\n  count(word, sort = TRUE)\n\nCreating a table to observe word counts\n\ncorpus_text &lt;- corpus(text_data)\nsummary(corpus_text, 5)\n\nCorpus consisting of 338 documents, showing 5 documents:\n\n                                   Text Types Tokens Sentences\n Alvarez PLC__0__0__Haacklee Herald.txt   206    433        18\n    Alvarez PLC__0__0__Lomark Daily.txt   102    170        12\n   Alvarez PLC__0__0__The News Buoy.txt    90    200         9\n Alvarez PLC__0__1__Haacklee Herald.txt    96    187         8\n    Alvarez PLC__0__1__Lomark Daily.txt   241    504        21\n\n\nTo separate the data; with 2 columns X & Y. Some text are “1” hence the split does not occur\n\ntext_data_splitted &lt;- text_data %&gt;%\n  separate_wider_delim(\"doc_id\",\n                       delim = \"__0__\",\n                       names = c(\"X\", \"Y\"),\n                       too_few = \"align_end\")\n\n\npacman::p_load(jsonlite, tidyverse)\n\n\n##pacman::p_load(jsonlite, tidygraph,\n##ggraph, tidyverse, readtext,\n               ##quanteda, tidytext)\n\nIn the code chunk below, fromJSON() of jsonlite package is used to import MC3.json into R environment.\n\nmc1_data &lt;- fromJSON(\"MC1/mc1.json\")\n\nmc2_data &lt;- fromJSON(“MC2/mc2.json”)\nmc3_data &lt;- fromJSON(“MC3/mc3.json”)"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html",
    "href": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "pacman::p_load(tidytext, widyr, wordcloud, DT, ggwordcloud, textplot, lubridate, hms,\ntidyverse, tidygraph, ggraph, igraph)\n\n\n\n\n\n\n\n\nnews20 &lt;- \"data/20news/\"\n\n\n\n\n\nread_folder &lt;- function(infolder) {\n  tibble(file = dir(infolder, \n                    full.names = TRUE)) %&gt;%\n    mutate(text = map(file, \n                      read_lines)) %&gt;%\n    transmute(id = basename(file), \n              text) %&gt;%\n    unnest(text)\n}\n\n\n\n\n\n\n\n\nraw_text &lt;- tibble(folder = \n                     dir(news20, \n                         full.names = TRUE)) %&gt;%\n  mutate(folder_out = map(folder, \n                          read_folder)) %&gt;%\n  unnest(cols = c(folder_out)) %&gt;%\n  transmute(newsgroup = basename(folder), \n            id, text)\nwrite_rds(raw_text, \"data/20news.rds\")\n\n\n\n\n\nnews20 &lt;- read_rds(\"data/news20.rds\")\n\nraw_text &lt;- news20\nraw_text %&gt;%\n  group_by(newsgroup) %&gt;%\n  summarize(messages = n_distinct(id)) %&gt;%\n  ggplot(aes(messages, newsgroup)) +\n  geom_col(fill = \"lightblue\") +\n  labs(y = NULL)\n\n\n\n\n\n\n\n\n\n\n\ncleaned_text &lt;- raw_text %&gt;%\n  group_by(newsgroup, id) %&gt;%\n  filter(cumsum(text == \"\") &gt; 0,\n         cumsum(str_detect(\n           text, \"^--\")) == 0) %&gt;%\n  ungroup()\n\n\n\n\n\ncleaned_text &lt;- cleaned_text %&gt;%\n  filter(str_detect(text, \"^[^&gt;]+[A-Za-z\\\\d]\")\n         | text == \"\",\n         !str_detect(text, \n                     \"writes(:|\\\\.\\\\.\\\\.)$\"),\n         !str_detect(text, \n                     \"^In article &lt;\")\n  )\n\n\n\n\n\nusenet_words &lt;- cleaned_text %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n\nusenet_words %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 5,542 × 2\n   word           n\n   &lt;chr&gt;      &lt;int&gt;\n 1 people        57\n 2 time          50\n 3 jesus         47\n 4 god           44\n 5 message       40\n 6 br            27\n 7 bible         23\n 8 drive         23\n 9 homosexual    23\n10 read          22\n# ℹ 5,532 more rows\n\nwords_by_newsgroup &lt;- usenet_words %&gt;%\n  count(newsgroup, word, sort = TRUE) %&gt;%\n  ungroup()\n\n\n\n\n\nwordcloud(words_by_newsgroup$word,\n          words_by_newsgroup$n,\n          max.words = 300)\n\n\n\n\n\n\n\n\n# set.seed(1234)\n# \n# words_by_newsgroup %&gt;%\n#   filter(n &gt; 0) %&gt;%\n# ggplot(aes(label = word,\n#           size = n)) +\n#   geom_text_wordcloud() +\n#   theme_minimal() +\n#   facet_wrap(~newsgroup)\n\n\n\n\n\n\n\n\ntf_idf &lt;- words_by_newsgroup %&gt;%\n  bind_tf_idf(word, newsgroup, n) %&gt;%\n  arrange(desc(tf_idf))\n\n\n\n\n\nDT::datatable(tf_idf, filter = 'top') %&gt;% \n  formatRound(columns = c('tf', 'idf', \n                          'tf_idf'), \n              digits = 3) %&gt;%\n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n\n\n\n\n\n\n\n\n\n\nDT::datatable(tf_idf, filter = 'top') %&gt;% \n  formatRound(columns = c('tf', 'idf', \n                          'tf_idf'), \n              digits = 3) %&gt;%\n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n\n\n\n\n\n\n\n\n\n\ntf_idf %&gt;%\n  filter(str_detect(newsgroup, \"^sci\\\\.\")) %&gt;%\n  group_by(newsgroup) %&gt;%\n  slice_max(tf_idf, \n            n = 12) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, \n                        tf_idf)) %&gt;%\n  ggplot(aes(tf_idf, \n             word, \n             fill = newsgroup)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ newsgroup, \n             scales = \"free\") +\n  labs(x = \"tf-idf\", \n       y = NULL)\n\n\n\n\n\n\n\n\nnewsgroup_cors &lt;- words_by_newsgroup %&gt;%\n  pairwise_cor(newsgroup, \n               word, \n               n, \n               sort = TRUE)\n\n\n\n\n\nset.seed(2017)\n\nnewsgroup_cors %&gt;%\n  filter(correlation &gt; .025) %&gt;%\n  graph_from_data_frame() %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha = correlation, \n                     width = correlation)) +\n  geom_node_point(size = 6, \n                  color = \"lightblue\") +\n  geom_node_text(aes(label = name),\n                 color = \"red\",\n                 repel = TRUE) +\n  theme_void()\n\n\n\n\n\n\n\n\nbigrams &lt;- cleaned_text %&gt;%\n  unnest_tokens(bigram, \n                text, \n                token = \"ngrams\", \n                n = 2)\n\nbigrams\n\n# A tibble: 28,827 × 3\n   newsgroup   id    bigram    \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;     \n 1 alt.atheism 54256 &lt;NA&gt;      \n 2 alt.atheism 54256 &lt;NA&gt;      \n 3 alt.atheism 54256 as i      \n 4 alt.atheism 54256 i don't   \n 5 alt.atheism 54256 don't know\n 6 alt.atheism 54256 know this \n 7 alt.atheism 54256 this book \n 8 alt.atheism 54256 book i    \n 9 alt.atheism 54256 i will    \n10 alt.atheism 54256 will use  \n# ℹ 28,817 more rows\n\n\n\n\n\n\nbigrams_count &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  count(bigram, sort = TRUE)\n\nbigrams_count\n\n# A tibble: 19,888 × 2\n   bigram       n\n   &lt;chr&gt;    &lt;int&gt;\n 1 of the     169\n 2 in the     113\n 3 to the      74\n 4 to be       59\n 5 for the     52\n 6 i have      48\n 7 that the    47\n 8 if you      40\n 9 on the      39\n10 it is       38\n# ℹ 19,878 more rows\n\n\n\n\n\n\nbigrams_separated &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), \n           sep = \" \")\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\nbigrams_filtered\n\n# A tibble: 4,607 × 4\n   newsgroup   id    word1        word2        \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        \n 1 alt.atheism 54256 defines      god          \n 2 alt.atheism 54256 term         preclues     \n 3 alt.atheism 54256 science      ideas        \n 4 alt.atheism 54256 ideas        drawn        \n 5 alt.atheism 54256 supernatural precludes    \n 6 alt.atheism 54256 scientific   assertions   \n 7 alt.atheism 54256 religious    dogma        \n 8 alt.atheism 54256 religion     involves     \n 9 alt.atheism 54256 involves     circumventing\n10 alt.atheism 54256 gain         absolute     \n# ℹ 4,597 more rows\n\n\n\n\n\n\nbigram_counts &lt;- bigrams_filtered %&gt;% \n  count(word1, word2, sort = TRUE)\n\n\n\n\n\nbigram_graph &lt;- bigram_counts %&gt;%\n  filter(n &gt; 3) %&gt;%\n  graph_from_data_frame()\nbigram_graph\n\nIGRAPH f8a55d1 DN-- 40 24 -- \n+ attr: name (v/c), n (e/n)\n+ edges from f8a55d1 (vertex names):\n [1] 1          -&gt;2           1          -&gt;3           static     -&gt;void       \n [4] time       -&gt;pad         1          -&gt;4           infield    -&gt;fly        \n [7] mat        -&gt;28          vv         -&gt;vv          1          -&gt;5          \n[10] cock       -&gt;crow        noticeshell-&gt;widget      27         -&gt;1993       \n[13] 3          -&gt;4           child      -&gt;molestation cock       -&gt;crew       \n[16] gun        -&gt;violence    heat       -&gt;sink        homosexual -&gt;male       \n[19] homosexual -&gt;women       include    -&gt;xol         mary       -&gt;magdalene  \n[22] read       -&gt;write       rev        -&gt;20          tt         -&gt;ee         \n\n\n\n\n\n\nset.seed(1234)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1)\n\n\n\n\n\n\n\n\nset.seed(1234)\n\na &lt;- grid::arrow(type = \"closed\", \n                 length = unit(.15,\n                               \"inches\"))\n\nggraph(bigram_graph, \n       layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), \n                 show.legend = FALSE,\n                 arrow = a, \n                 end_cap = circle(.07,\n                                  'inches')) +\n  geom_node_point(color = \"lightblue\", \n                  size = 5) +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1) +\n  theme_void()\n\n\n\n\n\n\n\n\nReference guide widyr: Widen, process, and re-tidy a dataset United Nations Voting Correlations"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#getting-started",
    "href": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#getting-started",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "pacman::p_load(tidytext, widyr, wordcloud, DT, ggwordcloud, textplot, lubridate, hms,\ntidyverse, tidygraph, ggraph, igraph)"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders",
    "href": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "news20 &lt;- \"data/20news/\"\n\n\n\n\n\nread_folder &lt;- function(infolder) {\n  tibble(file = dir(infolder, \n                    full.names = TRUE)) %&gt;%\n    mutate(text = map(file, \n                      read_lines)) %&gt;%\n    transmute(id = basename(file), \n              text) %&gt;%\n    unnest(text)\n}"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders-1",
    "href": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#importing-multiple-text-files-from-multiple-folders-1",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "raw_text &lt;- tibble(folder = \n                     dir(news20, \n                         full.names = TRUE)) %&gt;%\n  mutate(folder_out = map(folder, \n                          read_folder)) %&gt;%\n  unnest(cols = c(folder_out)) %&gt;%\n  transmute(newsgroup = basename(folder), \n            id, text)\nwrite_rds(raw_text, \"data/20news.rds\")\n\n\n\n\n\nnews20 &lt;- read_rds(\"data/news20.rds\")\n\nraw_text &lt;- news20\nraw_text %&gt;%\n  group_by(newsgroup) %&gt;%\n  summarize(messages = n_distinct(id)) %&gt;%\n  ggplot(aes(messages, newsgroup)) +\n  geom_col(fill = \"lightblue\") +\n  labs(y = NULL)"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#introducing-tidytext",
    "href": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#introducing-tidytext",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "cleaned_text &lt;- raw_text %&gt;%\n  group_by(newsgroup, id) %&gt;%\n  filter(cumsum(text == \"\") &gt; 0,\n         cumsum(str_detect(\n           text, \"^--\")) == 0) %&gt;%\n  ungroup()\n\n\n\n\n\ncleaned_text &lt;- cleaned_text %&gt;%\n  filter(str_detect(text, \"^[^&gt;]+[A-Za-z\\\\d]\")\n         | text == \"\",\n         !str_detect(text, \n                     \"writes(:|\\\\.\\\\.\\\\.)$\"),\n         !str_detect(text, \n                     \"^In article &lt;\")\n  )\n\n\n\n\n\nusenet_words &lt;- cleaned_text %&gt;%\n  unnest_tokens(word, text) %&gt;%\n  filter(str_detect(word, \"[a-z']$\"),\n         !word %in% stop_words$word)\n\nusenet_words %&gt;%\n  count(word, sort = TRUE)\n\n# A tibble: 5,542 × 2\n   word           n\n   &lt;chr&gt;      &lt;int&gt;\n 1 people        57\n 2 time          50\n 3 jesus         47\n 4 god           44\n 5 message       40\n 6 br            27\n 7 bible         23\n 8 drive         23\n 9 homosexual    23\n10 read          22\n# ℹ 5,532 more rows\n\nwords_by_newsgroup &lt;- usenet_words %&gt;%\n  count(newsgroup, word, sort = TRUE) %&gt;%\n  ungroup()\n\n\n\n\n\nwordcloud(words_by_newsgroup$word,\n          words_by_newsgroup$n,\n          max.words = 300)\n\n\n\n\n\n\n\n\n# set.seed(1234)\n# \n# words_by_newsgroup %&gt;%\n#   filter(n &gt; 0) %&gt;%\n# ggplot(aes(label = word,\n#           size = n)) +\n#   geom_text_wordcloud() +\n#   theme_minimal() +\n#   facet_wrap(~newsgroup)"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#basic-concept-of-tf-idf",
    "href": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#basic-concept-of-tf-idf",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "tf_idf &lt;- words_by_newsgroup %&gt;%\n  bind_tf_idf(word, newsgroup, n) %&gt;%\n  arrange(desc(tf_idf))\n\n\n\n\n\nDT::datatable(tf_idf, filter = 'top') %&gt;% \n  formatRound(columns = c('tf', 'idf', \n                          'tf_idf'), \n              digits = 3) %&gt;%\n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n\n\n\n\n\n\n\n\n\n\nDT::datatable(tf_idf, filter = 'top') %&gt;% \n  formatRound(columns = c('tf', 'idf', \n                          'tf_idf'), \n              digits = 3) %&gt;%\n  formatStyle(0, \n              target = 'row', \n              lineHeight='25%')\n\n\n\n\n\n\n\n\n\n\ntf_idf %&gt;%\n  filter(str_detect(newsgroup, \"^sci\\\\.\")) %&gt;%\n  group_by(newsgroup) %&gt;%\n  slice_max(tf_idf, \n            n = 12) %&gt;%\n  ungroup() %&gt;%\n  mutate(word = reorder(word, \n                        tf_idf)) %&gt;%\n  ggplot(aes(tf_idf, \n             word, \n             fill = newsgroup)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ newsgroup, \n             scales = \"free\") +\n  labs(x = \"tf-idf\", \n       y = NULL)\n\n\n\n\n\n\n\n\nnewsgroup_cors &lt;- words_by_newsgroup %&gt;%\n  pairwise_cor(newsgroup, \n               word, \n               n, \n               sort = TRUE)\n\n\n\n\n\nset.seed(2017)\n\nnewsgroup_cors %&gt;%\n  filter(correlation &gt; .025) %&gt;%\n  graph_from_data_frame() %&gt;%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(alpha = correlation, \n                     width = correlation)) +\n  geom_node_point(size = 6, \n                  color = \"lightblue\") +\n  geom_node_text(aes(label = name),\n                 color = \"red\",\n                 repel = TRUE) +\n  theme_void()\n\n\n\n\n\n\n\n\nbigrams &lt;- cleaned_text %&gt;%\n  unnest_tokens(bigram, \n                text, \n                token = \"ngrams\", \n                n = 2)\n\nbigrams\n\n# A tibble: 28,827 × 3\n   newsgroup   id    bigram    \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;     \n 1 alt.atheism 54256 &lt;NA&gt;      \n 2 alt.atheism 54256 &lt;NA&gt;      \n 3 alt.atheism 54256 as i      \n 4 alt.atheism 54256 i don't   \n 5 alt.atheism 54256 don't know\n 6 alt.atheism 54256 know this \n 7 alt.atheism 54256 this book \n 8 alt.atheism 54256 book i    \n 9 alt.atheism 54256 i will    \n10 alt.atheism 54256 will use  \n# ℹ 28,817 more rows\n\n\n\n\n\n\nbigrams_count &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  count(bigram, sort = TRUE)\n\nbigrams_count\n\n# A tibble: 19,888 × 2\n   bigram       n\n   &lt;chr&gt;    &lt;int&gt;\n 1 of the     169\n 2 in the     113\n 3 to the      74\n 4 to be       59\n 5 for the     52\n 6 i have      48\n 7 that the    47\n 8 if you      40\n 9 on the      39\n10 it is       38\n# ℹ 19,878 more rows\n\n\n\n\n\n\nbigrams_separated &lt;- bigrams %&gt;%\n  filter(bigram != 'NA') %&gt;%\n  separate(bigram, c(\"word1\", \"word2\"), \n           sep = \" \")\n\nbigrams_filtered &lt;- bigrams_separated %&gt;%\n  filter(!word1 %in% stop_words$word) %&gt;%\n  filter(!word2 %in% stop_words$word)\n\nbigrams_filtered\n\n# A tibble: 4,607 × 4\n   newsgroup   id    word1        word2        \n   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;        \n 1 alt.atheism 54256 defines      god          \n 2 alt.atheism 54256 term         preclues     \n 3 alt.atheism 54256 science      ideas        \n 4 alt.atheism 54256 ideas        drawn        \n 5 alt.atheism 54256 supernatural precludes    \n 6 alt.atheism 54256 scientific   assertions   \n 7 alt.atheism 54256 religious    dogma        \n 8 alt.atheism 54256 religion     involves     \n 9 alt.atheism 54256 involves     circumventing\n10 alt.atheism 54256 gain         absolute     \n# ℹ 4,597 more rows\n\n\n\n\n\n\nbigram_counts &lt;- bigrams_filtered %&gt;% \n  count(word1, word2, sort = TRUE)\n\n\n\n\n\nbigram_graph &lt;- bigram_counts %&gt;%\n  filter(n &gt; 3) %&gt;%\n  graph_from_data_frame()\nbigram_graph\n\nIGRAPH f8a55d1 DN-- 40 24 -- \n+ attr: name (v/c), n (e/n)\n+ edges from f8a55d1 (vertex names):\n [1] 1          -&gt;2           1          -&gt;3           static     -&gt;void       \n [4] time       -&gt;pad         1          -&gt;4           infield    -&gt;fly        \n [7] mat        -&gt;28          vv         -&gt;vv          1          -&gt;5          \n[10] cock       -&gt;crow        noticeshell-&gt;widget      27         -&gt;1993       \n[13] 3          -&gt;4           child      -&gt;molestation cock       -&gt;crew       \n[16] gun        -&gt;violence    heat       -&gt;sink        homosexual -&gt;male       \n[19] homosexual -&gt;women       include    -&gt;xol         mary       -&gt;magdalene  \n[22] read       -&gt;write       rev        -&gt;20          tt         -&gt;ee         \n\n\n\n\n\n\nset.seed(1234)\n\nggraph(bigram_graph, layout = \"fr\") +\n  geom_edge_link() +\n  geom_node_point() +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1)\n\n\n\n\n\n\n\n\nset.seed(1234)\n\na &lt;- grid::arrow(type = \"closed\", \n                 length = unit(.15,\n                               \"inches\"))\n\nggraph(bigram_graph, \n       layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n), \n                 show.legend = FALSE,\n                 arrow = a, \n                 end_cap = circle(.07,\n                                  'inches')) +\n  geom_node_point(color = \"lightblue\", \n                  size = 5) +\n  geom_node_text(aes(label = name), \n                 vjust = 1, \n                 hjust = 1) +\n  theme_void()"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#references",
    "href": "Hand-on_EX/Hand-on-Ex05/Hands-on_Ex05.html#references",
    "title": "Hands-on_Ex05",
    "section": "",
    "text": "Reference guide widyr: Widen, process, and re-tidy a dataset United Nations Voting Correlations"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html",
    "href": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, you will be able to:\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph, build network graph visualisation using appropriate functions of ggraph, compute network geometrics using tidygraph, build advanced graph visualisation by incorporating the network geometrics, and build interactive network visualisation using visNetwork package\n\n\n\n\n\nIn this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)\n\n\n\n\n\nThe data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees.\n\n\n\n\nIn this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\nRows: 9,063 Columns: 10 $ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26… $ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29… $ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"… $ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0… $ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP… $ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela… $ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr… $ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc… $ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0… $ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\nRows: 1,372 Columns: 4 $ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… $ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,… $ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu… $ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…\n\n\n\n\nIn this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 星期日       5\n2     1     2 星期一       2\n3     1     2 星期二       3\n# ℹ 1,369 more rows\n\n\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n 1    40    41 星期六      13\n 2    41    43 星期一      11\n 3    35    31 星期二      10\n 4    40    41 星期一      10\n 5    40    43 星期一      10\n 6    36    32 星期日       9\n 7    40    43 星期六       9\n 8    41    40 星期一       9\n 9    19    15 星期三       8\n10    35    38 星期二       8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function.\n\n\n\n\nggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\n\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\nhings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line.\n\n\n\n\n\nAnother very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\nCentrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()\n\n\n\n\n\n\n\n\n\nvisNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\nCode\nvisNetwork(GAStech_nodes,            GAStech_edges_aggregated) %&gt;%   visIgraphLayout(layout = \"layout_with_fr\") \nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\nCode\nGAStech_nodes &lt;- GAStech_nodes %&gt;%   rename(group = Department) \nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\nCode\nvisNetwork(GAStech_nodes,            GAStech_edges_aggregated) %&gt;%   visIgraphLayout(layout = \"layout_with_fr\") %&gt;%   visLegend() %&gt;%   visLayout(randomSeed = 123)\n\n\n\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\nCode\nvisNetwork(GAStech_nodes,            GAStech_edges_aggregated) %&gt;%   visIgraphLayout(layout = \"layout_with_fr\") %&gt;%   visEdges(arrows = \"to\",             smooth = list(enabled = TRUE,                           type = \"curvedCW\")) %&gt;%   visLegend() %&gt;%   visLayout(randomSeed = 123)\nVisit Option to find out more about visEdges’s argument.\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\nCode\nvisNetwork(GAStech_nodes,            GAStech_edges_aggregated) %&gt;%   visIgraphLayout(layout = \"layout_with_fr\") %&gt;%   visOptions(highlightNearest = TRUE,              nodesIdSelection = TRUE) %&gt;%   visLegend() %&gt;%   visLayout(randomSeed = 123)\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#overview",
    "href": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#overview",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "In this hands-on exercise, you will learn how to model, analyse and visualise network data using R.\nBy the end of this hands-on exercise, you will be able to:\ncreate graph object data frames, manipulate them using appropriate functions of dplyr, lubridate, and tidygraph, build network graph visualisation using appropriate functions of ggraph, compute network geometrics using tidygraph, build advanced graph visualisation by incorporating the network geometrics, and build interactive network visualisation using visNetwork package"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#getting-started",
    "href": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#getting-started",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "In this hands-on exercise, four network data modelling and visualisation packages will be installed and launched. They are igraph, tidygraph, ggraph and visNetwork. Beside these four packages, tidyverse and lubridate, an R package specially designed to handle and wrangling time data will be installed and launched too.\nThe code chunk:\n\npacman::p_load(igraph, tidygraph, ggraph, \n               visNetwork, lubridate, clock,\n               tidyverse, graphlayouts)"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#the-data",
    "href": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#the-data",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "The data sets used in this hands-on exercise is from an oil exploration and extraction company. There are two data sets. One contains the nodes data and the other contains the edges (also know as link) data.\n\n\nGAStech-email_edges.csv which consists of two weeks of 9063 emails correspondances between 55 employees.\n\n\n\nGAStech_email_nodes.csv which consist of the names, department and title of the 55 employees."
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#importing-network-data-from-files",
    "href": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#importing-network-data-from-files",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "In this step, you will import GAStech_email_node.csv and GAStech_email_edges-v2.csv into RStudio environment by using read_csv() of readr package\n\nGAStech_nodes &lt;- read_csv(\"data/GAStech_email_node.csv\")\nGAStech_edges &lt;- read_csv(\"data/GAStech_email_edge-v2.csv\")\n\n\n\nNext, we will examine the structure of the data frame using glimpse() of dplyr.\n\nglimpse(GAStech_edges)\n\nRows: 9,063\nColumns: 8\n$ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26…\n$ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29…\n$ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"…\n$ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0…\n$ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP…\n$ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela…\n$ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr…\n$ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc…\n\n\n\n\n\nThe code chunk below will be used to perform the changes.\n\nGAStech_edges &lt;- GAStech_edges %&gt;%\n  mutate(SendDate = dmy(SentDate)) %&gt;%\n  mutate(Weekday = wday(SentDate,\n                        label = TRUE,\n                        abbr = FALSE))\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\nRows: 9,063 Columns: 10 $ source      &lt;dbl&gt; 43, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 26, 26, 26… $ target      &lt;dbl&gt; 41, 40, 51, 52, 53, 45, 44, 46, 48, 49, 47, 54, 27, 28, 29… $ SentDate    &lt;chr&gt; \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\", \"6/1/2014\"… $ SentTime    &lt;time&gt; 08:39:00, 08:39:00, 08:58:00, 08:58:00, 08:58:00, 08:58:0… $ Subject     &lt;chr&gt; \"GT-SeismicProcessorPro Bug Report\", \"GT-SeismicProcessorP… $ MainSubject &lt;chr&gt; \"Work related\", \"Work related\", \"Work related\", \"Work rela… $ sourceLabel &lt;chr&gt; \"Sven.Flecha\", \"Sven.Flecha\", \"Kanon.Herrero\", \"Kanon.Herr… $ targetLabel &lt;chr&gt; \"Isak.Baza\", \"Lucas.Alcazar\", \"Felix.Resumir\", \"Hideki.Coc… $ SendDate    &lt;date&gt; 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-06, 2014-01-0… $ Weekday     &lt;ord&gt; Friday, Friday, Friday, Friday, Friday, Friday, Friday, Fr…\n\n\n\nA close examination of GAStech_edges data.frame reveals that it consists of individual e-mail flow records. This is not very useful for visualisation.\nIn view of this, we will aggregate the individual by date, senders, receivers, main subject and day of the week.\nThe code chunk:\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(source, target, Weekday) %&gt;%\n    summarise(Weight = n()) %&gt;%\n  filter(source!=target) %&gt;%\n  filter(Weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nTable below shows the data structure of the reformatted GAStech_edges data frame\nRows: 1,372 Columns: 4 $ source  &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… $ target  &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 6,… $ Weekday &lt;ord&gt; Sunday, Monday, Tuesday, Wednesday, Friday, Sunday, Monday, Tu… $ Weight  &lt;int&gt; 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5, 2, 3, 4, 6, 5,…"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#creating-network-objects-using-tidygraph",
    "href": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#creating-network-objects-using-tidygraph",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "In this section, you will learn how to create a graph data model by using tidygraph package. It provides a tidy API for graph/network manipulation. While network data itself is not tidy, it can be envisioned as two tidy tables, one for node data and one for edge data. tidygraph provides a way to switch between the two tables and provides dplyr verbs for manipulating them. Furthermore it provides access to a lot of graph algorithms with return values that facilitate their use in a tidy workflow.\nBefore getting started, you are advised to read these two articles:\n\nIntroducing tidygraph\ntidygraph 1.1 - A tidy hope\n\n\n\nTwo functions of tidygraph package can be used to create network objects, they are:\n\ntbl_graph() creates a tbl_graph network object from nodes and edges data.\nas_tbl_graph() converts network data and objects to a tbl_graph network. Below are network data and objects supported by as_tbl_graph()\n\na node data.frame and an edge data.frame,\ndata.frame, list, matrix from base,\nigraph from igraph,\nnetwork from network,\ndendrogram and hclust from stats,\nNode from data.tree,\nphylo and evonet from ape, and\ngraphNEL, graphAM, graphBAM from graph (in Bioconductor).\n\n\n\n\n\n\nactivate() verb from tidygraph serves as a switch between tibbles for nodes and edges. All dplyr verbs applied to tbl_graph object are applied to the active tibble.\n\n\n\nIn the above the .N() function is used to gain access to the node data while manipulating the edge data. Similarly .E() will give you the edge data and .G() will give you the tbl_graph object itself.\n\n\n\n\nIn this section, you will use tbl_graph() of tinygraph package to build an tidygraph’s network graph data.frame.\nBefore typing the codes, you are recommended to review to reference guide of tbl_graph()\n\nGAStech_graph &lt;- tbl_graph(nodes = GAStech_nodes,\n                           edges = GAStech_edges_aggregated, \n                           directed = TRUE)\n\n\n\n\n\nGAStech_graph\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Node Data: 54 × 4 (active)\n      id label               Department     Title                               \n   &lt;dbl&gt; &lt;chr&gt;               &lt;chr&gt;          &lt;chr&gt;                               \n 1     1 Mat.Bramar          Administration Assistant to CEO                    \n 2     2 Anda.Ribera         Administration Assistant to CFO                    \n 3     3 Rachel.Pantanal     Administration Assistant to CIO                    \n 4     4 Linda.Lagos         Administration Assistant to COO                    \n 5     5 Ruscella.Mies.Haber Administration Assistant to Engineering Group Mana…\n 6     6 Carla.Forluniau     Administration Assistant to IT Group Manager       \n 7     7 Cornelia.Lais       Administration Assistant to Security Group Manager \n 8    44 Kanon.Herrero       Security       Badging Office                      \n 9    45 Varja.Lagos         Security       Badging Office                      \n10    46 Stenig.Fusil        Security       Building Control                    \n# ℹ 44 more rows\n#\n# Edge Data: 1,372 × 4\n   from    to Weekday Weight\n  &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n1     1     2 星期日       5\n2     1     2 星期一       2\n3     1     2 星期二       3\n# ℹ 1,369 more rows\n\n\n\n\n\n\nThe output above reveals that GAStech_graph is a tbl_graph object with 54 nodes and 4541 edges.\nThe command also prints the first six rows of “Node Data” and the first three of “Edge Data”.\nIt states that the Node Data is active. The notion of an active tibble within a tbl_graph object makes it possible to manipulate the data in one tibble at a time.\n\n\n\n\nThe nodes tibble data frame is activated by default, but you can change which tibble data frame is active with the activate() function. Thus, if we wanted to rearrange the rows in the edges tibble to list those with the highest “weight” first, we could use activate() and then arrange().\nFor example,\n\nGAStech_graph %&gt;%\n  activate(edges) %&gt;%\n  arrange(desc(Weight))\n\n# A tbl_graph: 54 nodes and 1372 edges\n#\n# A directed multigraph with 1 component\n#\n# Edge Data: 1,372 × 4 (active)\n    from    to Weekday Weight\n   &lt;int&gt; &lt;int&gt; &lt;ord&gt;    &lt;int&gt;\n 1    40    41 星期六      13\n 2    41    43 星期一      11\n 3    35    31 星期二      10\n 4    40    41 星期一      10\n 5    40    43 星期一      10\n 6    36    32 星期日       9\n 7    40    43 星期六       9\n 8    41    40 星期一       9\n 9    19    15 星期三       8\n10    35    38 星期二       8\n# ℹ 1,362 more rows\n#\n# Node Data: 54 × 4\n     id label           Department     Title           \n  &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt;          &lt;chr&gt;           \n1     1 Mat.Bramar      Administration Assistant to CEO\n2     2 Anda.Ribera     Administration Assistant to CFO\n3     3 Rachel.Pantanal Administration Assistant to CIO\n# ℹ 51 more rows\n\n\nVisit the reference guide of activate() to find out more about the function."
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#plotting-static-network-graphs-with-ggraph-package",
    "href": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#plotting-static-network-graphs-with-ggraph-package",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "ggraph is an extension of ggplot2, making it easier to carry over basic ggplot skills to the design of network graphs.\nAs in all network graph, there are three main aspects to a ggraph’s network graph, they are:\n\nnodes,\nedges and\nlayouts.\n\nFor a comprehensive discussion of each of this aspect of graph, please refer to their respective vignettes provided.\n\n\nThe code chunk below uses ggraph(), geom-edge_link() and geom_node_point() to plot a network graph by using GAStech_graph. Before your get started, it is advisable to read their respective reference guide at least once.\n\nggraph(GAStech_graph) +\n  geom_edge_link() +\n  geom_node_point()\n\n\n\n\n\n\n\nIn this section, you will use theme_graph() to remove the x and y axes. Before your get started, it is advisable to read it’s reference guide at least once.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\n\n\n\nFurthermore, theme_graph() makes it easy to change the coloring of the plot.\n\ng &lt;- ggraph(GAStech_graph) + \n  geom_edge_link(aes(colour = 'grey50')) +\n  geom_node_point(aes(colour = 'grey40'))\n\ng + theme_graph(background = 'grey10',\n                text_colour = 'white')\n\n\n\n\n\n\n\nggraph support many layout for standard used, they are: star, circle, nicely (default), dh, gem, graphopt, grid, mds, spahere, randomly, fr, kk, drl and lgl. Figures below and on the right show layouts supported by ggraph().\n\n\n\nThe code chunks below will be used to plot the network graph using Fruchterman and Reingold layout.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"fr\") +\n  geom_edge_link(aes()) +\n  geom_node_point(aes())\n\ng + theme_graph()\n\n\n\n\nThing to learn from the code chunk above:\n\nlayout argument is used to define the layout to be used.\n\n\n\n\nIn this section, you will colour each node by referring to their respective departments.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes()) +\n  geom_node_point(aes(colour = Department, \n                      size = 3))\n\ng + theme_graph()\n\n\n\n\nhings to learn from the code chunks above:\n\ngeom_node_point is equivalent in functionality to geo_point of ggplot2. It allows for simple plotting of nodes in different shapes, colours and sizes. In the codes chnuks above colour and size are used.\n\n\n\n\nIn the code chunk below, the thickness of the edges will be mapped with the Weight variable.\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") +\n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 3)\n\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunks above:\n\ngeom_edge_link draws edges in the simplest way - as straight lines between the start and end nodes. But, it can do more that that. In the example above, argument width is used to map the width of the line in proportional to the Weight attribute and argument alpha is used to introduce opacity on the line."
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#creating-facet-graphs",
    "href": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#creating-facet-graphs",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "Another very useful feature of ggraph is faceting. In visualising network data, this technique can be used to reduce edge over-plotting in a very meaning way by spreading nodes and edges out based on their attributes. In this section, you will learn how to use faceting technique to visualise network data.\nThere are three functions in ggraph to implement faceting, they are:\n\nfacet_nodes() whereby edges are only draw in a panel if both terminal nodes are present here,\nfacet_edges() whereby nodes are always drawn in al panels even if the node data contains an attribute named the same as the one used for the edge facetting, and\nfacet_graph() faceting on two variables simultaneously.\n\n\n\nIn the code chunk below, facet_edges() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n\ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below uses theme() to change the position of the legend.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2) +\n  theme(legend.position = 'bottom')\n  \ng + facet_edges(~Weekday)\n\n\n\n\n\n\n\nThe code chunk below adds frame to each graph.\n\nset_graph_style() \n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_edges(~Weekday) +\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\nIn the code chunkc below, facet_nodes() is used. Before getting started, it is advisable for you to read it’s reference guide at least once.\n\nset_graph_style()\n\ng &lt;- ggraph(GAStech_graph, \n            layout = \"nicely\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department), \n                  size = 2)\n  \ng + facet_nodes(~Department)+\n  th_foreground(foreground = \"grey80\",  \n                border = TRUE) +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#network-metrics-analysis",
    "href": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#network-metrics-analysis",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "Centrality measures are a collection of statistical indices use to describe the relative important of the actors are to a network. There are four well-known centrality measures, namely: degree, betweenness, closeness and eigenvector. It is beyond the scope of this hands-on exercise to cover the principles and mathematics of these measure here. Students are encouraged to refer to Chapter 7: Actor Prominence of A User’s Guide to Network Analysis in R to gain better understanding of theses network measures.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(betweenness_centrality = centrality_betweenness()) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department,\n            size=betweenness_centrality))\ng + theme_graph()\n\n\n\n\nThings to learn from the code chunk above:\n\nmutate() of dplyr is used to perform the computation.\nthe algorithm used, on the other hand, is the centrality_betweenness() of tidygraph.\n\n\n\n\nIt is important to note that from ggraph v2.0 onward tidygraph algorithms such as centrality measures can be accessed directly in ggraph calls. This means that it is no longer necessary to precompute and store derived node and edge centrality measures on the graph in order to use them in a plot.\n\ng &lt;- GAStech_graph %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = Department, \n                      size = centrality_betweenness()))\ng + theme_graph()\n\n\n\n\n\n\n\ntidygraph package inherits many of the community detection algorithms imbedded into igraph and makes them available to us, including Edge-betweenness (group_edge_betweenness), Leading eigenvector (group_leading_eigen), Fast-greedy (group_fast_greedy), Louvain (group_louvain), Walktrap (group_walktrap), Label propagation (group_label_prop), InfoMAP (group_infomap), Spinglass (group_spinglass), and Optimal (group_optimal). Some community algorithms are designed to take into account direction or weight, while others ignore it. Use this link to find out more about community detection functions provided by tidygraph,\nIn the code chunk below group_edge_betweenness() is used.\n\ng &lt;- GAStech_graph %&gt;%\n  mutate(community = as.factor(group_edge_betweenness(weights = Weight, directed = TRUE))) %&gt;%\n  ggraph(layout = \"fr\") + \n  geom_edge_link(aes(width=Weight), \n                 alpha=0.2) +\n  scale_edge_width(range = c(0.1, 5)) +\n  geom_node_point(aes(colour = community))  \n\ng + theme_graph()"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#building-interactive-network-graph-with-visnetwork",
    "href": "Hand-on_EX/Hand-on-EX06/Hand-on-EX06.html#building-interactive-network-graph-with-visnetwork",
    "title": "Hands-on_Ex06",
    "section": "",
    "text": "visNetwork() is a R package for network visualization, using vis.js javascript library.\nvisNetwork() function uses a nodes list and edges list to create an interactive graph.\n\nThe nodes list must include an “id” column, and the edge list must have “from” and “to” columns.\nThe function also plots the labels for the nodes, using the names of the actors from the “label” column in the node list.\n\nThe resulting graph is fun to play around with.\n\nYou can move the nodes and the graph will use an algorithm to keep the nodes properly spaced.\nYou can also zoom in and out on the plot and move it around to re-center it.\n\n\n\n\nBefore we can plot the interactive network graph, we need to prepare the data model by using the code chunk below.\n\nGAStech_edges_aggregated &lt;- GAStech_edges %&gt;%\n  left_join(GAStech_nodes, by = c(\"sourceLabel\" = \"label\")) %&gt;%\n  rename(from = id) %&gt;%\n  left_join(GAStech_nodes, by = c(\"targetLabel\" = \"label\")) %&gt;%\n  rename(to = id) %&gt;%\n  filter(MainSubject == \"Work related\") %&gt;%\n  group_by(from, to) %&gt;%\n    summarise(weight = n()) %&gt;%\n  filter(from!=to) %&gt;%\n  filter(weight &gt; 1) %&gt;%\n  ungroup()\n\n\n\n\nThe code chunk below will be used to plot an interactive network graph by using the data prepared.\n\nvisNetwork(GAStech_nodes, \n           GAStech_edges_aggregated)\n\n\n\n\n\n\n\n\nIn the code chunk below, Fruchterman and Reingold layout is used.\nCode\nvisNetwork(GAStech_nodes,            GAStech_edges_aggregated) %&gt;%   visIgraphLayout(layout = \"layout_with_fr\") \nVisit Igraph to find out more about visIgraphLayout’s argument.\n\n\n\nvisNetwork() looks for a field called “group” in the nodes object and colour the nodes according to the values of the group field.\nThe code chunk below rename Department field to group.\nCode\nGAStech_nodes &lt;- GAStech_nodes %&gt;%   rename(group = Department) \nWhen we rerun the code chunk below, visNetwork shades the nodes by assigning unique colour to each category in the group field.\nCode\nvisNetwork(GAStech_nodes,            GAStech_edges_aggregated) %&gt;%   visIgraphLayout(layout = \"layout_with_fr\") %&gt;%   visLegend() %&gt;%   visLayout(randomSeed = 123)\n\n\n\nIn the code run below visEdges() is used to symbolise the edges.\n- The argument arrows is used to define where to place the arrow.\n- The smooth argument is used to plot the edges using a smooth curve.\nCode\nvisNetwork(GAStech_nodes,            GAStech_edges_aggregated) %&gt;%   visIgraphLayout(layout = \"layout_with_fr\") %&gt;%   visEdges(arrows = \"to\",             smooth = list(enabled = TRUE,                           type = \"curvedCW\")) %&gt;%   visLegend() %&gt;%   visLayout(randomSeed = 123)\nVisit Option to find out more about visEdges’s argument.\n\n\n\nIn the code chunk below, visOptions() is used to incorporate interactivity features in the data visualisation.\n\nThe argument highlightNearest highlights nearest when clicking a node.\nThe argument nodesIdSelection adds an id node selection creating an HTML select element.\n\nCode\nvisNetwork(GAStech_nodes,            GAStech_edges_aggregated) %&gt;%   visIgraphLayout(layout = \"layout_with_fr\") %&gt;%   visOptions(highlightNearest = TRUE,              nodesIdSelection = TRUE) %&gt;%   visLegend() %&gt;%   visLayout(randomSeed = 123)\nVisit Option to find out more about visOption’s argument."
  },
  {
    "objectID": "In-class_EX/In-class_EX06/In-class_EX06.html",
    "href": "In-class_EX/In-class_EX06/In-class_EX06.html",
    "title": "In-class_EX06",
    "section": "",
    "text": "library(stringi)\n\nWarning: package 'stringi' was built under R version 4.3.3\n\nlibrary(stringr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(rvest)\n\nWarning: package 'rvest' was built under R version 4.3.3\n\nlibrary(corporaexplorer)\n\nWarning: package 'corporaexplorer' was built under R version 4.3.3\n\nlibrary(readr)\n\n\nAttaching package: 'readr'\n\n\nThe following object is masked from 'package:rvest':\n\n    guess_encoding"
  },
  {
    "objectID": "In-class_EX/In-class_EX06/In-class_EX06.html#corporaexplorer",
    "href": "In-class_EX/In-class_EX06/In-class_EX06.html#corporaexplorer",
    "title": "In-class_EX06",
    "section": "corporaexplorer",
    "text": "corporaexplorer\nWhen we first have a data frame with text and metadata, creating a “corporaexplorerobject” for exploration is very simple:\n\n# As this is a corpus which is not organised by date,\n  # we set `date_based_corpus` to `FALSE`.\n# Because we want to organise our exploration around the books in the Bible,\n  # we pass `\"Book\"` to the `grouping_variable` argument.\n# We specify which metadata columns we want to be displayed in the\n  # \"Document information\" tab, using the `columns_doc_info` argument.\nKJB &lt;- prepare_data(dataset = bible_df,\n                    date_based_corpus = FALSE,\n                    grouping_variable = \"Book\",\n                    columns_doc_info = c(\"Testament\", \"Book\"))\n\nStarting.\n\n\nDocument data frame done.\n\n\nCorpus is not date based. Calendar data frame skipped.\n\n\nDocument term matrix: text processed.\n\n\nDocument term matrix: tokenising completed.\n\n\nDocument term matrix: word list created.\n\n\nDocument term matrix done.\n\n\nDone.\n\n\n\nRun corpus explorer\n\nclass(KJB)\n\n[1] \"corporaexplorerobject\"\n\n\n\nexplore(KJB)\n\nExploring 1,175 documents\n\n\nLoading required package: shiny\n\n\nShiny applications not supported in static R Markdown documents\n\n\n\npacman::p_load(tidyverse, readtext,\n               quanteda, tidytext, jsonlite, tidygraph, ggraph, visNetwork, garphlayouts,ggforce, skimr)\n\nInstalling package into 'C:/Users/LENOVO/AppData/Local/R/win-library/4.3'\n(as 'lib' is unspecified)\n\n\nWarning: package 'garphlayouts' is not available for this version of R\n\nA version of this package for your version of R might be available elsewhere,\nsee the ideas at\nhttps://cran.r-project.org/doc/manuals/r-patched/R-admin.html#Installing-packages\n\n\nWarning: unable to access index for repository http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3:\n  cannot open URL 'http://www.stats.ox.ac.uk/pub/RWin/bin/windows/contrib/4.3/PACKAGES'\n\n\nWarning in p_install(package, character.only = TRUE, ...):\n\n\nWarning in library(package, lib.loc = lib.loc, character.only = TRUE,\nlogical.return = TRUE, : there is no package called 'garphlayouts'\n\n\nWarning in pacman::p_load(tidyverse, readtext, quanteda, tidytext, jsonlite, : Failed to install/load:\ngarphlayouts\n\n\n\nmc3_data &lt;- fromJSON(\"MC3/MC3.json\")"
  },
  {
    "objectID": "DSB/Time Serise EDA.html",
    "href": "DSB/Time Serise EDA.html",
    "title": "DSB",
    "section": "",
    "text": "library(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(data.table)\n\n\nAttaching package: 'data.table'\n\nThe following objects are masked from 'package:lubridate':\n\n    hour, isoweek, mday, minute, month, quarter, second, wday, week,\n    yday, year\n\nThe following objects are masked from 'package:dplyr':\n\n    between, first, last\n\nThe following object is masked from 'package:purrr':\n\n    transpose\n\nlibrary(ggplot2)\n\n# 读取数据\nfile_path &lt;- \"D:/transactions.csv\"  # 替换为你的文件路径\ndf &lt;- fread(file_path)\n\n# 解析日期\ndf$`Document Date` &lt;- as.Date(df$`Document Date`, format = \"%m/%d/%Y\")\n\n# 检查并移除无效日期\ndf &lt;- df %&gt;%\n  filter(!is.na(`Document Date`))\n\n# 检查日期范围\nstart_date &lt;- min(df$`Document Date`, na.rm = TRUE)\nend_date &lt;- max(df$`Document Date`, na.rm = TRUE)\n\nprint(paste(\"Date range:\", start_date, \"to\", end_date))\n\n[1] \"Date range: 2021-08-01 to 2023-07-31\"\n\n# 按日期聚合净价\ndf_daily &lt;- df %&gt;%\n  group_by(`Document Date`) %&gt;%\n  summarise(Net_Price = sum(`Net Price`, na.rm = TRUE))\n\n# 再次检查日期范围\nstart_date &lt;- min(df_daily$`Document Date`, na.rm = TRUE)\nend_date &lt;- max(df_daily$`Document Date`, na.rm = TRUE)\n\nprint(paste(\"Aggregated date range:\", start_date, \"to\", end_date))\n\n[1] \"Aggregated date range: 2021-08-01 to 2023-07-31\"\n\nif (is.finite(start_date) & is.finite(end_date)) {\n  # 填补缺失值\n  df_daily &lt;- df_daily %&gt;%\n    complete(`Document Date` = seq.Date(start_date, end_date, by = \"day\")) %&gt;%\n    fill(Net_Price, .direction = \"downup\")\n} else {\n  stop(\"Invalid date range detected. Please check your data.\")\n}\n\n# 绘制基础时间序列图\nggplot(df_daily, aes(x = `Document Date`, y = Net_Price)) +\n  geom_line() +\n  labs(title = \"Daily Net Price Over Time\", x = \"Date\", y = \"Net Price\") +\n  theme_minimal()\n\n\n\n# 计算Recency, Frequency, Monetary\ndf$Recency &lt;- as.numeric(difftime(max(df$`Document Date`), df$`Document Date`, units = \"days\"))\n\ndf_rfm &lt;- df %&gt;%\n  group_by(Customer) %&gt;%\n  summarise(\n    Recency = min(Recency, na.rm = TRUE),\n    Frequency = n_distinct(`Order No`),\n    Monetary = sum(`Net Price`, na.rm = TRUE)\n  )\n\nprint(head(df_rfm))\n\n# A tibble: 6 × 4\n  Customer Recency Frequency Monetary\n     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;    &lt;dbl&gt;\n1  1000003     693         1     46.7\n2  1000147     678         1     23.4\n3  1000298     587         1     34.6\n4  1000343     725         1    105. \n5  1000467      57         1    219. \n6  1000468     231         1    433. \n\n# 示例产品代码\narticle_code &lt;- \"A14Y010000\"  # 替换为你的产品代码\n\n# 筛选指定产品的数据\ndf_product &lt;- df %&gt;%\n  filter(`Article Code` == article_code) %&gt;%\n  group_by(`Document Date`, `Item Discount`) %&gt;%\n  summarise(Qty = sum(Qty, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'Document Date'. You can override using the\n`.groups` argument.\n\n# 绘制产品销售情况\nggplot(df_product, aes(x = `Document Date`, y = Qty)) +\n  geom_line() +\n  geom_point(data = df_product %&gt;% filter(`Item Discount` &gt; 0), aes(color = \"red\")) +\n  scale_color_manual(values = c(\"red\" = \"red\")) +\n  labs(title = paste(\"Time Series Chart for Article Code:\", article_code),\n       x = \"Document Date\", y = \"Qty\",\n       color = \"Item on Discount\") +\n  theme_minimal()\n\n\n\n# 计算每个产品是否增加销量\nresults_df &lt;- df %&gt;%\n  group_by(`Article Code`, `Item Discount`) %&gt;%\n  summarise(Total_Sales = sum(Qty, na.rm = TRUE)) %&gt;%\n  pivot_wider(names_from = `Item Discount`, values_from = Total_Sales, values_fill = 0) %&gt;%\n  mutate(Increase_in_Sales = `1` &gt; `0`) %&gt;%\n  ungroup() %&gt;%\n  group_by(Increase_in_Sales) %&gt;%\n  summarise(Count = n())\n\n`summarise()` has grouped output by 'Article Code'. You can override using the\n`.groups` argument.\n\nprint(head(results_df))\n\n# A tibble: 1 × 2\n  Increase_in_Sales Count\n  &lt;lgl&gt;             &lt;int&gt;\n1 FALSE               503\n\n# 绘制分析结果\nggplot(results_df, aes(x = Increase_in_Sales, y = Count, fill = Increase_in_Sales)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Sales Increase Analysis\", x = \"Increase in Sales\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(forecast)\n\nWarning: package 'forecast' was built under R version 4.3.3\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n# 读取清洗后的数据\nfile_path &lt;- \"D:/cleaned_transactions.csv\"  # 替换为清洗后数据的文件路径\ndf &lt;- fread(file_path)\n\n# 解析日期\ndf$`Document Date` &lt;- as.Date(df$`Document Date`, format = \"%Y-%m-%d\")\n\n# 检查并移除无效日期\ndf &lt;- df %&gt;%\n  filter(!is.na(`Document Date`))\n\n# 按日期聚合净价\ndf_daily &lt;- df %&gt;%\n  group_by(`Document Date`) %&gt;%\n  summarise(Net_Price = sum(`Net Price`, na.rm = TRUE))\n\n# 按日期和分类聚合净价\ndf_category_daily &lt;- df %&gt;%\n  group_by(`Document Date`, `Category Description`) %&gt;%\n  summarise(Net_Price = sum(`Net Price`, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'Document Date'. You can override using the\n`.groups` argument.\n\n# 按日期和销售渠道聚合净价\ndf_channel_daily &lt;- df %&gt;%\n  group_by(`Document Date`, `Sales Channel`) %&gt;%\n  summarise(Net_Price = sum(`Net Price`, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'Document Date'. You can override using the\n`.groups` argument.\n\n# 绘制整体时间序列图\nggplot(df_daily, aes(x = `Document Date`, y = Net_Price)) +\n  geom_line() +\n  labs(title = \"Daily Net Price Over Time\",\n       x = \"Date\", y = \"Net Price\") +\n  theme_minimal()\n\n\n\n# 绘制按分类的时间序列图\nggplot(df_category_daily, aes(x = `Document Date`, y = Net_Price, color = `Category Description`)) +\n  geom_line() +\n  labs(title = \"Daily Net Price Over Time by Category\",\n       x = \"Date\", y = \"Net Price\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n# 绘制按销售渠道的时间序列图\nggplot(df_channel_daily, aes(x = `Document Date`, y = Net_Price, color = `Sales Channel`)) +\n  geom_line() +\n  labs(title = \"Daily Net Price Over Time by Sales Channel\",\n       x = \"Date\", y = \"Net Price\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n# 按日期聚合销售数量\ndf_qty_daily &lt;- df %&gt;%\n  group_by(`Document Date`) %&gt;%\n  summarise(Quantity = sum(Qty, na.rm = TRUE))\n\n# 绘制销售数量时间序列图\nggplot(df_qty_daily, aes(x = `Document Date`, y = Quantity)) +\n  geom_line() +\n  labs(title = \"Daily Quantity Sold Over Time\",\n       x = \"Date\", y = \"Quantity Sold\") +\n  theme_minimal()\n\n\n\n# 按日期和折扣聚合净价\ndf_discount_daily &lt;- df %&gt;%\n  group_by(`Document Date`, Discount &gt; 0) %&gt;%\n  summarise(Net_Price = sum(`Net Price`, na.rm = TRUE)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Discount_Status = ifelse(`Discount &gt; 0`, \"With Discount\", \"Without Discount\"))\n\n`summarise()` has grouped output by 'Document Date'. You can override using the\n`.groups` argument.\n\n# 绘制按折扣状态的时间序列图\nggplot(df_discount_daily, aes(x = `Document Date`, y = Net_Price, color = Discount_Status)) +\n  geom_line() +\n  labs(title = \"Daily Net Price Over Time by Discount Status\",\n       x = \"Date\", y = \"Net Price\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n# 季节性分解\nts_net_price &lt;- ts(df_daily$Net_Price, frequency = 365)\ndecomp &lt;- decompose(ts_net_price)\nplot(decomp)\n\n\n\n\n\n# 按月聚合净价\ndf_monthly &lt;- df_daily %&gt;%\n  mutate(Month = floor_date(`Document Date`, \"month\")) %&gt;%\n  group_by(Month) %&gt;%\n  summarise(Net_Price = sum(Net_Price, na.rm = TRUE))\n\n# 创建时间序列对象，频率设为12（表示每年12个时间点，即按月）\nts_net_price_monthly &lt;- ts(df_monthly$Net_Price, start = c(year(min(df_monthly$Month)), month(min(df_monthly$Month))), frequency = 12)\n\n# ACF图\nggAcf(ts_net_price_monthly) +\n  labs(title = \"ACF of Net Price (Monthly)\",\n       x = \"Month\", y = \"ACF\") +\n  scale_x_continuous(breaks = seq(0, 24, 3), labels = function(x) format(seq.Date(from = min(df_monthly$Month), by = \"month\", length.out = 25)[x+1], \"%Y-%m\"))\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n# PACF图\nggPacf(ts_net_price_monthly) +\n  labs(title = \"PACF of Net Price (Monthly)\",\n       x = \"Month\", y = \"PACF\") +\n  scale_x_continuous(breaks = seq(0, 24, 3), labels = function(x) format(seq.Date(from = min(df_monthly$Month), by = \"month\", length.out = 25)[x+1], \"%Y-%m\"))\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\n\n\n\n\npacman::p_load(prophet,seasonal,forecast,gridExtra)\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(gridExtra)\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# 读取清洗后的数据\nfile_path &lt;- \"D:/cleaned_transactions.csv\"  # 替换为清洗后数据的文件路径\ndf &lt;- fread(file_path)\n\n# 解析日期\ndf$`Document Date` &lt;- as.Date(df$`Document Date`, format = \"%Y-%m-%d\")\n\n# 按日期聚合净价\ndf_daily &lt;- df %&gt;%\n  group_by(`Document Date`) %&gt;%\n  summarise(Net_Price = sum(`Net Price`, na.rm = TRUE))\n\n# 确保日期序列连续\ndf_daily &lt;- df_daily %&gt;%\n  complete(`Document Date` = seq.Date(min(`Document Date`), max(`Document Date`), by = \"day\")) %&gt;%\n  fill(Net_Price, .direction = \"downup\")\n\n# 创建时间序列对象，指定开始和结束日期\nstart_date &lt;- as.numeric(format(min(df_daily$`Document Date`), \"%Y\"))\nstart_month &lt;- as.numeric(format(min(df_daily$`Document Date`), \"%m\"))\nts_net_price &lt;- ts(df_daily$Net_Price, start = c(start_date, start_month), frequency = 365)\n\n# 季节性分解\ndecomp &lt;- decompose(ts_net_price)\n\n# 使用 ggplot2 绘制分解结果\ndecomp_df &lt;- data.frame(\n  date = seq.Date(from = min(df_daily$`Document Date`), by = \"day\", length.out = length(decomp$trend)),\n  observed = decomp$x,\n  trend = decomp$trend,\n  seasonal = decomp$seasonal,\n  random = decomp$random\n)\n\np1 &lt;- ggplot(decomp_df, aes(x = date, y = observed)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Observed\", x = \"Date\", y = \"Observed\") +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal()\n\np2 &lt;- ggplot(decomp_df, aes(x = date, y = trend)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Trend\", x = \"Date\", y = \"Trend\") +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal()\n\np3 &lt;- ggplot(decomp_df, aes(x = date, y = seasonal)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Seasonal\", x = \"Date\", y = \"Seasonal\") +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal()\n\np4 &lt;- ggplot(decomp_df, aes(x = date, y = random)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Random\", x = \"Date\", y = \"Random\") +\n  scale_y_continuous(labels = scales::comma) +\n  theme_minimal()\n\n# 使用 gridExtra 包将四个图放在一起\ngrid.arrange(p1, p2, ncol = 1)\n\nWarning: Removed 364 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(data.table)\n\n# 读取数据\nfile_path &lt;- \"D:/transactions.csv\"  # 替换为你的文件路径\ndf &lt;- fread(file_path)\n\n# 解析日期\ndf$`Document Date` &lt;- as.Date(df$`Document Date`, format = \"%m/%d/%Y\")\n\n# 新列：Item_Type\ndf &lt;- df %&gt;%\n  mutate(Item_Type = case_when(\n    `Net Price` == 0 & str_detect(Description, \"Sample|SP|SAMP|SMP\") ~ \"Sample\",\n    `Net Price` == 0 & str_detect(Description, \"GWP\") ~ \"GWP\",\n    TRUE ~ \"Paid\"\n  ))\n\n# 新列：Series\ndf &lt;- df %&gt;%\n  mutate(Series = case_when(\n    str_detect(Description, \"Invati\") ~ \"Invati\",\n    str_detect(Description, \"Botanical Repair\") ~ \"Botanical Repair\",\n    str_detect(Description, \"Colour Control\") ~ \"Colour Control\",\n    str_detect(Description, \"Rosemary Mint\") ~ \"Rosemary Mint\",\n    TRUE ~ \"Other\"\n  ))\n\n# 筛选 Sales_Transaction = 'S' 和 Item_Type = 'Paid' 的记录\ndf_filtered &lt;- df %&gt;%\n  filter(`Sales Transaction Type` == 'S' & Item_Type == 'Paid')\n\n# 查看数据清洗后的结果\nhead(df_filtered)\n\n     Order No Customer Sales Transaction Type Document Date Store Id\n1: 3006716497  3616853                      S    2021-08-01     6022\n2: 3006716497  3616853                      S    2021-08-01     6022\n3: 3006716497  3616853                      S    2021-08-01     6022\n4: 3006716498  2707866                      S    2021-08-01     5009\n5: 3006716498  2707866                      S    2021-08-01     5009\n6: 3006716498  2707866                      S    2021-08-01     5009\n   Sales Employee Id Article Code                           Description\n1:          20011149   AMFR010000       AVH INVATI ADV THICK COND 200ML\n2:          20011149   AMFW010000    AVH INVATI ADV SCALP REVITAL 150ML\n3:          20011149   AWLC010000   AVH INVATI ADV EXF SHAMP RICH 200ML\n4:          20011525   A46M010000          AVH MENS FIRM HOLD GEL 150ML\n5:          20011525   AKCH010000    AVH INVATI MEN SCALP REVITAL 125ML\n6:          20011525   AWK8010000 AVH INVATI ADV EXF SHAMP LIGHT 1000ML\n   Category Description Sales Channel Discount  Gst Item Discount Net Price Qty\n1:             Haircare        Retail        0 3.66           0.0     52.34   1\n2:             Haircare        Retail        0 7.99           0.0    114.01   1\n3:             Haircare        Retail        0 3.53           0.0     50.47   1\n4:             Haircare        Retail        0 3.27           0.0     46.73   1\n5:             Haircare        Retail        0 7.33           0.0    104.67   1\n6:             Haircare        Retail        0 9.73          37.2    139.07   1\n   Retail Price Item_Type Series\n1:           56      Paid  Other\n2:          122      Paid  Other\n3:           54      Paid  Other\n4:           50      Paid  Other\n5:          112      Paid  Other\n6:          186      Paid  Other\n\n# 按日期聚合净价\ndf_daily &lt;- df_filtered %&gt;%\n  group_by(`Document Date`) %&gt;%\n  summarise(Net_Price = sum(`Net Price`, na.rm = TRUE))\n\n# 绘制净价格为零的记录分布情况\nzero_net_price_records &lt;- df_filtered %&gt;%\n  filter(`Net Price` == 0)\n\n# 按销售渠道分组并计算数量\nchannel_summary &lt;- zero_net_price_records %&gt;%\n  group_by(`Sales Channel`) %&gt;%\n  summarise(Count = n())\n\n# 绘制条形图\nggplot(channel_summary, aes(x = `Sales Channel`, y = Count, fill = `Sales Channel`)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Zero Net Price Records by Sales Channel\",\n       x = \"Sales Channel\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(data.table)\nlibrary(ggridges)\n\nWarning: package 'ggridges' was built under R version 4.3.3\n\nlibrary(scales)\n\n# 解析日期\ndf$`Document Date` &lt;- as.Date(df$`Document Date`, format = \"%Y-%m-%d\")\n\n# 按日期和分类聚合净价\ndf_category_daily &lt;- df %&gt;%\n  filter(`Sales Transaction Type` == \"S\", `Item_Type` == \"Paid\") %&gt;%\n  group_by(`Document Date`, `Category Description`) %&gt;%\n  summarise(Net_Price = sum(`Net Price`, na.rm = TRUE), .groups = 'drop') %&gt;%\n  ungroup()\n\n# 将 Net_Price 转换为对数尺度\ndf_category_daily$Log_Net_Price &lt;- log1p(df_category_daily$Net_Price)\n\n# 创建山脊图\nridgeline_combined &lt;- ggplot(df_category_daily, aes(x = Log_Net_Price, y = `Category Description`, fill = `Category Description`)) +\n  geom_density_ridges(alpha = 0.5, scale = 1, rel_min_height = 0.01) +\n  scale_fill_viridis_d() +\n  theme_ridges() +\n  labs(title = \"Ridgeline Plot of Net Price by Category\",\n       x = \"Net Price\", y = NULL) +\n  scale_x_continuous(labels = NULL, breaks = NULL) +  # 移除x轴标签\n  theme(legend.position = \"right\", legend.text = element_text(size = 10))\n\nprint(ridgeline_combined)\n\nPicking joint bandwidth of 0.215\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(scales)  # 用于逗号格式化\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(scales)  # 用于逗号格式化\n\n\n# 解析日期\ndf$`Document Date` &lt;- as.Date(df$`Document Date`, format = \"%m/%d/%Y\")\n\n# 检查并移除无效日期\ndf &lt;- df %&gt;%\n  filter(!is.na(`Document Date`))\n\n# 提取年月\ndf &lt;- df %&gt;%\n  mutate(YearMonth = floor_date(`Document Date`, \"month\"))\n\n# 按年月和销售渠道聚合净价\ndf_channel_monthly &lt;- df %&gt;%\n  group_by(YearMonth, `Sales Channel`) %&gt;%\n  summarise(Net_Price = sum(`Net Price`, na.rm = TRUE)) %&gt;%\n  ungroup()\n\n`summarise()` has grouped output by 'YearMonth'. You can override using the\n`.groups` argument.\n\n# 绘制按销售渠道的月度时间序列图，增加Y轴的可读性\nggplot(df_channel_monthly, aes(x = YearMonth, y = Net_Price, color = `Sales Channel`)) +\n  geom_line() +\n  labs(title = \"Monthly Net Price Over Time by Sales Channel\",\n       x = \"Date\", y = \"Net Price\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_y_continuous(labels = comma) +  # 使用逗号格式化Y轴数值\n  scale_x_date(date_labels = \"%Y-%m\", date_breaks = \"2 month\")  # 设置X轴显示格式和间隔\n\n\n\n\n\nlibrary(dplyr)\nlibrary(lubridate)\n\n# 加载数据\ntransactions_df &lt;- read.csv(\"D:/cleaned_transactions.csv\")\n\n# 转换日期列为日期类型\ntransactions_df$Document.Date &lt;- as.Date(transactions_df$Document.Date)\n\n# 移除 Net.Price = 0 的行\ntransactions_df &lt;- transactions_df %&gt;%\n  filter(Net.Price != 0)\n\n# 按顾客和购买日期排序\ntransactions_df &lt;- transactions_df %&gt;%\n  arrange(Customer, Document.Date)\n\n# 计算每个顾客对每个物品的购买时间差\ntransactions_df &lt;- transactions_df %&gt;%\n  group_by(Customer, Article.Code) %&gt;%\n  mutate(Time.Diff = c(NA, diff(Document.Date)))\n\n# 为每次购买添加购买索引\ntransactions_df &lt;- transactions_df %&gt;%\n  group_by(Customer, Article.Code) %&gt;%\n  mutate(Purchase.Index = row_number()) %&gt;%\n  ungroup() %&gt;%\n  select(Customer, Article.Code, Purchase.Index, Document.Date, Time.Diff)\n\n# 存储详细分析结果为CSV文件\noutput_file_path &lt;- file.path(getwd(), \"cleaned_transactions_no_zero_net_price.csv\")\nwrite.csv(transactions_df, output_file_path, row.names = FALSE)\n\n# 检查输出文件路径\noutput_file_path\n\n[1] \"D:/cbzhang2023/ISS608.VAA1/DSB/cleaned_transactions_no_zero_net_price.csv\"\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Load the data\ncleaned_data &lt;- read.csv(\"D:/cleaned_transactions.csv\")\n\n# Convert 'Document Date' to Date format\ncleaned_data$Document.Date &lt;- as.Date(cleaned_data$Document.Date)\n\n# Aggregate data\nmonthly_data &lt;- cleaned_data %&gt;%\n  mutate(Month = format(Document.Date, \"%Y-%m\")) %&gt;%\n  group_by(Month, Sales.Channel) %&gt;%\n  summarise(Total.Net.Price = sum(Net.Price, na.rm = TRUE)) %&gt;%\n  spread(Sales.Channel, Total.Net.Price, fill = 0)\n\n`summarise()` has grouped output by 'Month'. You can override using the\n`.groups` argument.\n\n# Plot total order value\nmonthly_data_long &lt;- monthly_data %&gt;%\n  gather(key = \"Sales.Channel\", value = \"Total.Net.Price\", -Month)\n\nggplot(monthly_data_long, aes(x = Month, y = Total.Net.Price, fill = Sales.Channel)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Total Order Value by Sales Channel\",\n       x = \"Month\",\n       y = \"Total Order Value ($000)\")\n\n\n\n# Plot share of total order value\nmonthly_data_long &lt;- monthly_data %&gt;%\n  gather(key = \"Sales.Channel\", value = \"Total.Net.Price\", -Month) %&gt;%\n  group_by(Month) %&gt;%\n  mutate(Share = Total.Net.Price / sum(Total.Net.Price))\n\nggplot(monthly_data_long, aes(x = Month, y = Share, fill = Sales.Channel)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Share of Total Order Value by Sales Channel\",\n       x = \"Month\",\n       y = \"Share of Total Order Value\")\n\n\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Load the data\ncleaned_data &lt;- read.csv(\"D:/cleaned_transactions.csv\")\n\n# Remove rows where Net.Price or Retail.Price is 0\ncleaned_data &lt;- cleaned_data %&gt;%\n  filter(Net.Price != 0, Retail.Price != 0)\n\n# Convert 'Document Date' to Date format\ncleaned_data$Document.Date &lt;- as.Date(cleaned_data$Document.Date)\n\n# Aggregate data\nmonthly_data &lt;- cleaned_data %&gt;%\n  mutate(Month = format(Document.Date, \"%Y-%m\")) %&gt;%\n  group_by(Month, Sales.Channel) %&gt;%\n  summarise(Total.Net.Price = sum(Net.Price, na.rm = TRUE)) %&gt;%\n  spread(Sales.Channel, Total.Net.Price, fill = 0)\n\n`summarise()` has grouped output by 'Month'. You can override using the\n`.groups` argument.\n\n# Define colors to match the target image\ncolors &lt;- c(\"Retail\" = \"#A8D08D\", \"eCommerce Stores\" = \"#F4B084\",\n            \"3rd Party Marketplaces\" = \"#9DC3E6\", \"Corp Sales\" = \"#FFD966\",\n            \"Multi-Channel\" = \"#C6E0B4\")\n\n# Plot total order value\nmonthly_data_long &lt;- monthly_data %&gt;%\n  gather(key = \"Sales.Channel\", value = \"Total.Net.Price\", -Month)\n\nggplot(monthly_data_long, aes(x = Month, y = Total.Net.Price, fill = Sales.Channel)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = colors) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Total Order Value by Sales Channel\",\n       x = \"Month\",\n       y = \"Total Order Value ($000)\") +\n  scale_y_continuous(labels = scales::comma)\n\n\n\n# Plot share of total order value\nmonthly_data_long &lt;- monthly_data %&gt;%\n  gather(key = \"Sales.Channel\", value = \"Total.Net.Price\", -Month) %&gt;%\n  group_by(Month) %&gt;%\n  mutate(Share = Total.Net.Price / sum(Total.Net.Price))\n\nggplot(monthly_data_long, aes(x = Month, y = Share, fill = Sales.Channel)) +\n  geom_bar(stat = \"identity\") +\n  scale_fill_manual(values = colors) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Share of Total Order Value by Sales Channel\",\n       x = \"Month\",\n       y = \"Share of Total Order Value\") +\n  scale_y_continuous(labels = scales::percent)\n\n\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Load the data\ncleaned_data &lt;- read.csv(\"D:/cleaned_transactions.csv\")\n\n# Remove rows where Net.Price or Retail.Price is 0\ncleaned_data &lt;- cleaned_data %&gt;%\n  filter(Net.Price != 0, Retail.Price != 0)\n\n# Convert 'Document Date' to Date format\ncleaned_data$Document.Date &lt;- as.Date(cleaned_data$Document.Date)\n\n# Aggregate data\nmonthly_data &lt;- cleaned_data %&gt;%\n  mutate(Month = format(Document.Date, \"%Y-%m\")) %&gt;%\n  group_by(Month, Sales.Channel) %&gt;%\n  summarise(\n    Total.Qty = sum(Qty, na.rm = TRUE),\n    Total.Orders = n_distinct(Order.No),\n    Total.Net.Price = sum(Net.Price, na.rm = TRUE)\n  ) %&gt;%\n  gather(key = \"Metric\", value = \"Value\", -Month, -Sales.Channel)\n\n`summarise()` has grouped output by 'Month'. You can override using the\n`.groups` argument.\n\n# Define colors to match the target image\ncolors &lt;- c(\"Retail\" = \"#A8D08D\", \"eCommerce Stores\" = \"#F4B084\",\n            \"3rd Party Marketplaces\" = \"#9DC3E6\", \"Corp Sales\" = \"#FFD966\",\n            \"Multi-Channel\" = \"#C6E0B4\")\n\n# Plot Qty of haircare products by Sales Channel\nggplot(monthly_data %&gt;% filter(Metric == \"Total.Qty\"), aes(x = Month, y = Value, fill = Sales.Channel)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  scale_fill_manual(values = colors) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Qty of haircare products by Sales Channel\",\n       x = \"Month\",\n       y = \"Sum of Qty\")\n\n\n\n# Plot Order no. of haircare products by Sales Channel\nggplot(monthly_data %&gt;% filter(Metric == \"Total.Orders\"), aes(x = Month, y = Value, fill = Sales.Channel)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  scale_fill_manual(values = colors) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Order no. of haircare products by Sales Channel\",\n       x = \"Month\",\n       y = \"Count of Order No\")\n\n\n\n# Plot Revenue of haircare products by Sales Channel\nggplot(monthly_data %&gt;% filter(Metric == \"Total.Net.Price\"), aes(x = Month, y = Value, fill = Sales.Channel)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +\n  scale_fill_manual(values = colors) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title = \"Revenue of haircare products by Sales Channel\",\n       x = \"Month\",\n       y = \"Sum of Net Price\")"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX07/Hands-on_EX07.html",
    "href": "Hand-on_EX/Hand-on-EX07/Hands-on_EX07.html",
    "title": "Hands-on EX07",
    "section": "",
    "text": "PUBLISHED\nDecember 4, 2023\nMODIFIED\nFebruary 11, 2024\n\n\nBy the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart\n\n\n\n\n\n\n\nWrite a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)\n\n\n\n\nIn this section, you will learn how to plot a calender heatmap programmatically by using ggplot2 package.\nBy the end of this section, you will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n\nFirst, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nNA\n20\n\n\nAfrica/Cairo\nTW\nNA\n6\n\n\nAfrica/Cairo\nTW\nNA\n8\n\n\nAfrica/Cairo\nCN\nNA\n11\n\n\nAfrica/Cairo\nUS\nNA\n15\n\n\nAfrica/Cairo\nCA\nNA\n11\n\n\n\n\n\n\n\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\n\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package."
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX07/Hands-on_EX07.html#learning-outcome",
    "href": "Hand-on_EX/Hand-on-EX07/Hands-on_EX07.html#learning-outcome",
    "title": "Hands-on EX07",
    "section": "",
    "text": "By the end of this hands-on exercise you will be able create the followings data visualisation by using R packages:\n\nplotting a calender heatmap by using ggplot2 functions,\nplotting a cycle plot by using ggplot2 function,\nplotting a slopegraph\nplotting a horizon chart"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX07/Hands-on_EX07.html#do-it-yourself",
    "href": "Hand-on_EX/Hand-on-EX07/Hands-on_EX07.html#do-it-yourself",
    "title": "Hands-on EX07",
    "section": "",
    "text": "Write a code chunk to check, install and launch the following R packages: scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table and tidyverse.\n\npacman::p_load(scales, viridis, lubridate, ggthemes, gridExtra, readxl, knitr, data.table, CGPfunctions, ggHoriPlot, tidyverse)"
  },
  {
    "objectID": "Hand-on_EX/Hand-on-EX07/Hands-on_EX07.html#plotting-calendar-heatmap",
    "href": "Hand-on_EX/Hand-on-EX07/Hands-on_EX07.html#plotting-calendar-heatmap",
    "title": "Hands-on EX07",
    "section": "",
    "text": "In this section, you will learn how to plot a calender heatmap programmatically by using ggplot2 package.\nBy the end of this section, you will be able to:\n\nplot a calender heatmap by using ggplot2 functions and extension,\nto write function using R programming,\nto derive specific date and time related field by using base R and lubridate packages\nto perform data preparation task by using tidyr and dplyr packages.\n\n\n\nFor the purpose of this hands-on exercise, eventlog.csv file will be used. This data file consists of 199,999 rows of time-series cyber attack records by country.\n\n\n\nFirst, you will use the code chunk below to import eventlog.csv file into R environment and called the data frame as attacks.\n\nattacks &lt;- read_csv(\"data/eventlog.csv\")\n\n\n\n\nIt is always a good practice to examine the imported data frame before further analysis is performed.\nFor example, kable() can be used to review the structure of the imported data frame.\n\nkable(head(attacks))\n\n\n\n\ntimestamp\nsource_country\ntz\n\n\n\n\n2015-03-12 15:59:16\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:00:48\nFR\nEurope/Paris\n\n\n2015-03-12 16:02:26\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:02:38\nUS\nAmerica/Chicago\n\n\n2015-03-12 16:03:22\nCN\nAsia/Shanghai\n\n\n2015-03-12 16:03:45\nCN\nAsia/Shanghai\n\n\n\n\n\nThere are three columns, namely timestamp, source_country and tz.\n\ntimestamp field stores date-time values in POSIXct format.\nsource_country field stores the source of the attack. It is in ISO 3166-1 alpha-2 country code.\ntz field stores time zone of the source IP address.\n\n\n\n\nStep 1: Deriving weekday and hour of day fields\nBefore we can plot the calender heatmap, two new fields namely wkday and hour need to be derived. In this step, we will write a function to perform the task.\n\nmake_hr_wkday &lt;- function(ts, sc, tz) {\n  real_times &lt;- ymd_hms(ts, \n                        tz = tz[1], \n                        quiet = TRUE)\n  dt &lt;- data.table(source_country = sc,\n                   wkday = weekdays(real_times),\n                   hour = hour(real_times))\n  return(dt)\n  }\n\nStep 2: Deriving the attacks tibble data frame\n\nwkday_levels &lt;- c('Saturday', 'Friday', \n                  'Thursday', 'Wednesday', \n                  'Tuesday', 'Monday', \n                  'Sunday')\n\nattacks &lt;- attacks %&gt;%\n  group_by(tz) %&gt;%\n  do(make_hr_wkday(.$timestamp, \n                   .$source_country, \n                   .$tz)) %&gt;% \n  ungroup() %&gt;% \n  mutate(wkday = factor(\n    wkday, levels = wkday_levels),\n    hour  = factor(\n      hour, levels = 0:23))\n\nTable below shows the tidy tibble table after processing.\n\nkable(head(attacks))\n\n\n\n\ntz\nsource_country\nwkday\nhour\n\n\n\n\nAfrica/Cairo\nBG\nNA\n20\n\n\nAfrica/Cairo\nTW\nNA\n6\n\n\nAfrica/Cairo\nTW\nNA\n8\n\n\nAfrica/Cairo\nCN\nNA\n11\n\n\nAfrica/Cairo\nUS\nNA\n15\n\n\nAfrica/Cairo\nCA\nNA\n11\n\n\n\n\n\n\n\n\n\ngrouped &lt;- attacks %&gt;% \n  count(wkday, hour) %&gt;% \n  ungroup() %&gt;%\n  na.omit()\n\nggplot(grouped, \n       aes(hour, \n           wkday, \n           fill = n)) + \ngeom_tile(color = \"white\", \n          size = 0.1) + \ntheme_tufte(base_family = \"Helvetica\") + \ncoord_equal() +\nscale_fill_gradient(name = \"# of attacks\",\n                    low = \"sky blue\", \n                    high = \"dark blue\") +\nlabs(x = NULL, \n     y = NULL, \n     title = \"Attacks by weekday and time of day\") +\ntheme(axis.ticks = element_blank(),\n      plot.title = element_text(hjust = 0.5),\n      legend.title = element_text(size = 8),\n      legend.text = element_text(size = 6) )\n\n\n\n\n\na tibble data table called grouped is derived by aggregating the attack by wkday and hour fields.\na new field called n is derived by using group_by() and count() functions.\nna.omit() is used to exclude missing value.\ngeom_tile() is used to plot tiles (grids) at each x and y position. color and size arguments are used to specify the border color and line size of the tiles.\ntheme_tufte() of ggthemes package is used to remove unnecessary chart junk. To learn which visual components of default ggplot2 have been excluded, you are encouraged to comment out this line to examine the default plot.\ncoord_equal() is used to ensure the plot will have an aspect ratio of 1:1.\nscale_fill_gradient() function is used to creates a two colour gradient (low-high).\n\nThen we can simply group the count by hour and wkday and plot it, since we know that we have values for every combination there’s no need to further preprocess the data.\n\n\n\nChallenge: Building multiple heatmaps for the top four countries with the highest number of attacks.\n\n\n\nStep 1: Deriving attack by country object\nIn order to identify the top 4 countries with the highest number of attacks, you are required to do the followings:\n\ncount the number of attacks by country,\ncalculate the percent of attackes by country, and\nsave the results in a tibble data frame.\n\n\nattacks_by_country &lt;- count(\n  attacks, source_country) %&gt;%\n  mutate(percent = percent(n/sum(n))) %&gt;%\n  arrange(desc(n))\n\nStep 2: Preparing the tidy data frame\nIn this step, you are required to extract the attack records of the top 4 countries from attacks data frame and save the data in a new tibble data frame (i.e. top4_attacks).\n\ntop4 &lt;- attacks_by_country$source_country[1:4]\ntop4_attacks &lt;- attacks %&gt;%\n  filter(source_country %in% top4) %&gt;%\n  count(source_country, wkday, hour) %&gt;%\n  ungroup() %&gt;%\n  mutate(source_country = factor(\n    source_country, levels = top4)) %&gt;%\n  na.omit()\n\n\n\n\nStep 3: Plotting the Multiple Calender Heatmap by using ggplot2 package."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "",
    "text": "In this exercise, we will be tacking Mini-case 3 of VAST Challenge 2024.\nWe will answer Questions 1 and 2 from the mini-challenge:\n\nFishEye analysts want to better visualize changes in corporate structures over time. Create a visual analytics approach that analysts can use to highlight temporal patterns and changes in corporate structures. Examine the most active people and businesses using visual analytics.\nUsing your visualizations, find and display examples of typical and atypical business transactions (e.g., mergers, acquisitions, etc.). Can you infer the motivations behind changes in their activity?\n\n\n\nWe will utilize the dataset provided by the VAST Challenge, which comprises network data representing various entities within the fishing industry. The nodes in this dataset symbolize different entities involved in the fishing business, while the edges represent the relationships and interactions between these entities.\n\n\n\nThe initial approach involves identifying abnormalities in the data, followed by necessary data wrangling and cleaning. We will conduct initial data exploration and analysis to understand the dataset better. Once this is complete, we will proceed to address each task individually. \nTask 1: Visualize Changes in Corporate Structures Over Time \nFor this task, we are planning to create network graphs of the corporate network that would change over time as new events occur. The main visualization techniques are still under discussion, but the primary purpose is to highlight temporal changes in corporate structures based on individuals related to these corporations and potentially their familial relationships. Currently, the idea is to have a visualization that looks like an evolving network graph with a time slider to represent temporal changes. This will allow analysts to dynamically observe how corporate relationships evolve over time and identify significant shifts or anomalies. \nTask 2: Analyze Business Transactions \nFor this task, we will focus on business transactions such as mergers or acquisitions. We plan to look at the edges representing beneficial ownership or shareholdership and analyze the start and end dates to determine which transactions are typical and which are atypical. The visualization approach involves using edge weights to indicate the significance of the transaction, with thicker lines representing more substantial transactions, such as beneficial ownership. This will help in visualizing the types of events and identifying unusual transaction patterns that could indicate suspicious activities."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#the-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#the-data",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "",
    "text": "We will utilize the dataset provided by the VAST Challenge, which comprises network data representing various entities within the fishing industry. The nodes in this dataset symbolize different entities involved in the fishing business, while the edges represent the relationships and interactions between these entities."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#loading-packages",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#loading-packages",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "2.1 Loading Packages",
    "text": "2.1 Loading Packages\n\n\nCode\npacman::p_load(jsonlite, tidygraph, ggraph, visNetwork, graphlayouts, ggforce, tidyr, dplyr,skimr, tidytext, tidyverse, scales, wordcloud, tm, treemap,lubridate,ggplot2)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#loading-data",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#loading-data",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "2.2 Loading Data",
    "text": "2.2 Loading Data\n\n\nCode\nmc3_data &lt;- fromJSON(\"mc3.json\")\nglimpse(mc3_data)\n\n\nList of 5\n $ directed  : logi TRUE\n $ multigraph: logi TRUE\n $ graph     : Named list()\n $ nodes     :'data.frame': 60520 obs. of  15 variables:\n  ..$ type             : chr [1:60520] \"Entity.Organization.Company\" \"Entity.Organization.Company\" \"Entity.Organization.Company\" \"Entity.Organization.Company\" ...\n  ..$ country          : chr [1:60520] \"Uziland\" \"Mawalara\" \"Uzifrica\" \"Islavaragon\" ...\n  ..$ ProductServices  : chr [1:60520] \"Unknown\" \"Furniture and home accessories\" \"Food products\" \"Unknown\" ...\n  ..$ PointOfContact   : chr [1:60520] \"Rebecca Lewis\" \"Michael Lopez\" \"Steven Robertson\" \"Anthony Wyatt\" ...\n  ..$ HeadOfOrg        : chr [1:60520] \"Émilie-Susan Benoit\" \"Honoré Lemoine\" \"Jules Labbé\" \"Dr. Víctor Hurtado\" ...\n  ..$ founding_date    : chr [1:60520] \"1954-04-24T00:00:00\" \"2009-06-12T00:00:00\" \"2029-12-15T00:00:00\" \"1972-02-16T00:00:00\" ...\n  ..$ revenue          : num [1:60520] 5995 71767 0 0 4747 ...\n  ..$ TradeDescription : chr [1:60520] \"Unknown\" \"Abbott-Gomez is a leading manufacturer and supplier of high-quality furniture and home accessories, catering to\"| __truncated__ \"Abbott-Harrison is a leading manufacturer of high-quality food products, including baked goods, snacks, and bev\"| __truncated__ \"Unknown\" ...\n  ..$ _last_edited_by  : chr [1:60520] \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" ...\n  ..$ _last_edited_date: chr [1:60520] \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _date_added      : chr [1:60520] \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _raw_source      : chr [1:60520] \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" ...\n  ..$ _algorithm       : chr [1:60520] \"Automatic Import\" \"Automatic Import\" \"Automatic Import\" \"Automatic Import\" ...\n  ..$ id               : chr [1:60520] \"Abbott, Mcbride and Edwards\" \"Abbott-Gomez\" \"Abbott-Harrison\" \"Abbott-Ibarra\" ...\n  ..$ dob              : chr [1:60520] NA NA NA NA ...\n $ links     :'data.frame': 75817 obs. of  11 variables:\n  ..$ start_date       : chr [1:75817] \"2016-10-29T00:00:00\" \"2035-06-03T00:00:00\" \"2028-11-20T00:00:00\" \"2024-09-04T00:00:00\" ...\n  ..$ type             : chr [1:75817] \"Event.Owns.Shareholdership\" \"Event.Owns.Shareholdership\" \"Event.Owns.Shareholdership\" \"Event.Owns.Shareholdership\" ...\n  ..$ _last_edited_by  : chr [1:75817] \"Pelagia Alethea Mordoch\" \"Niklaus Oberon\" \"Pelagia Alethea Mordoch\" \"Pelagia Alethea Mordoch\" ...\n  ..$ _last_edited_date: chr [1:75817] \"2035-01-01T00:00:00\" \"2035-07-15T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _date_added      : chr [1:75817] \"2035-01-01T00:00:00\" \"2035-07-15T00:00:00\" \"2035-01-01T00:00:00\" \"2035-01-01T00:00:00\" ...\n  ..$ _raw_source      : chr [1:75817] \"Existing Corporate Structure Data\" \"Oceanus Corporations Monthly - Jun '35\" \"Existing Corporate Structure Data\" \"Existing Corporate Structure Data\" ...\n  ..$ _algorithm       : chr [1:75817] \"Automatic Import\" \"Manual Entry\" \"Automatic Import\" \"Automatic Import\" ...\n  ..$ source           : chr [1:75817] \"Avery Inc\" \"Berger-Hayes\" \"Bowers Group\" \"Bowman-Howe\" ...\n  ..$ target           : chr [1:75817] \"Allen, Nichols and Thompson\" \"Jensen, Morris and Downs\" \"Barnett Inc\" \"Bennett Ltd\" ...\n  ..$ key              : int [1:75817] 0 0 0 0 0 0 0 0 0 0 ...\n  ..$ end_date         : chr [1:75817] NA NA NA NA ..."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#data-clean",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#data-clean",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "2.3 Data Clean",
    "text": "2.3 Data Clean\nThis step filters the columns of interest from the edges and nodes data frames. For edges, it keeps the columns type, source, and target, which are essential for defining relationships in the network. For nodes, it keeps the columns type and id, which uniquely identify each node and its type.\n\n\nCode\nmc3_edges &lt;- as_tibble(mc3_data$link)\nmc3_nodes &lt;- as_tibble(mc3_data$nodes)\nmc3_edges_filt &lt;- mc3_edges %&gt;% select(type, source, target)\nmc3_nodes_filt &lt;- mc3_nodes %&gt;% select(type, id)\n\n\nSkimming the Data\n\n\nCode\nskim(mc3_edges)\n\n\n\nData summary\n\n\nName\nmc3_edges\n\n\nNumber of rows\n75817\n\n\nNumber of columns\n11\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n10\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nstart_date\n90\n1\n10\n19\n0\n19228\n0\n\n\ntype\n0\n1\n14\n31\n0\n4\n0\n\n\n_last_edited_by\n0\n1\n14\n23\n0\n2\n0\n\n\n_last_edited_date\n0\n1\n19\n19\n0\n13\n0\n\n\n_date_added\n0\n1\n19\n19\n0\n13\n0\n\n\n_raw_source\n0\n1\n33\n38\n0\n13\n0\n\n\n_algorithm\n0\n1\n12\n16\n0\n2\n0\n\n\nsource\n0\n1\n6\n42\n0\n51996\n0\n\n\ntarget\n0\n1\n6\n48\n0\n8926\n0\n\n\nend_date\n75469\n0\n19\n19\n0\n73\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nkey\n0\n1\n0.21\n0.41\n0\n0\n0\n0\n2\n▇▁▂▁▁\n\n\n\n\n\n\n\nCode\nskim(mc3_nodes)\n\n\n\nData summary\n\n\nName\nmc3_nodes\n\n\nNumber of rows\n60520\n\n\nNumber of columns\n15\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n14\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ntype\n0\n1.00\n13\n36\n0\n8\n0\n\n\ncountry\n0\n1.00\n4\n15\n0\n86\n0\n\n\nProductServices\n51649\n0.15\n1\n1737\n0\n3219\n0\n\n\nPointOfContact\n51665\n0.15\n3\n29\n0\n8533\n0\n\n\nHeadOfOrg\n51649\n0.15\n3\n47\n0\n8796\n0\n\n\nfounding_date\n51649\n0.15\n19\n19\n0\n7716\n0\n\n\nTradeDescription\n51649\n0.15\n6\n1129\n0\n4124\n0\n\n\n_last_edited_by\n0\n1.00\n12\n23\n0\n3\n0\n\n\n_last_edited_date\n0\n1.00\n19\n19\n0\n15\n0\n\n\n_date_added\n0\n1.00\n19\n19\n0\n13\n0\n\n\n_raw_source\n0\n1.00\n33\n38\n0\n13\n0\n\n\n_algorithm\n0\n1.00\n12\n16\n0\n2\n0\n\n\nid\n0\n1.00\n6\n48\n0\n60520\n0\n\n\ndob\n8871\n0.85\n10\n19\n0\n14999\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nrevenue\n51665\n0.15\n278631.9\n4698536\n0\n0\n8233.59\n26340.55\n310612303\n▇▁▁▁▁"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#overall-of-eda",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#overall-of-eda",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "2.4 Overall of EDA",
    "text": "2.4 Overall of EDA\nIn this section, we conduct an exploratory data analysis (EDA) to gain insights into the characteristics and distribution of the dataset. The EDA focuses on the temporal distribution of company founding dates and transaction start dates, as well as the distribution of entity types in the dataset.\n\nTemporal Distribution of Companies and Transactions\nTo understand the temporal distribution of the companies’ founding dates and transaction start dates, we utilize histograms to visualize the data. The following code snippet creates these histograms: ::: panel-tabset ## Node\n\n\nCode\nmc3_nodes %&gt;%\n  ggplot(aes(x = as.Date(founding_date, format = \"%Y-%m-%dT%H:%M:%S\"))) +\n  geom_histogram(binwidth = 5 * 365, fill = \"blue\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"TIME ANALYSIS\", x = \"DATA\", y = \"NUMBERT OF COMPANIES\")"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#data-wrangling",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#data-wrangling",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "3.1 Data Wrangling",
    "text": "3.1 Data Wrangling\nThe initial steps extract and separate lists from the edges table to handle any embedded list structures within the source column. This ensures that each relationship is correctly represented as a single row. Afterward, the edge data is cleaned by removing any remaining list structures and combining the cleaned edges. The most active nodes are identified by counting the transactions associated with each source node, then selecting the top 10 based on transaction count. Finally, the edges and nodes are filtered to include only those that involve the most active nodes, ensuring that the network visualization focuses on the key entities and their relationships\n\n\nCode\nmc3_edge_unclean &lt;- mc3_edges %&gt;%\n  filter(substr(source, 1, 2) == \"c(\")\n\nmc3_edge_broken &lt;- mc3_edge_unclean %&gt;%\n  separate_rows(source, sep = \"\\\\(|\\\\,|\\\\)\") %&gt;%\n  mutate(source = gsub(\"\\\"\", \"\", source)) %&gt;%\n  filter(source != \"c\") %&gt;%\n  mutate(source = trimws(source)) %&gt;%\n  mutate(target = trimws(target)) %&gt;%\n  group_by(source, target, type) %&gt;%\n  summarise(weights = n(), .groups = 'drop') %&gt;%\n  filter(source != target)\n\nmc3_edges_without_list &lt;- mc3_edges %&gt;%\n  filter(!substr(source, 1, 2) == \"c(\") %&gt;%\n  distinct()\n\nmc3_edges_combined &lt;- bind_rows(mc3_edges_without_list, mc3_edge_broken)\n\nactive_nodes &lt;- mc3_edges_combined %&gt;%\n  group_by(source) %&gt;%\n  summarise(transaction_count = n()) %&gt;%\n  arrange(desc(transaction_count)) %&gt;%\n  top_n(10, transaction_count)\n\nfiltered_edges &lt;- mc3_edges_combined %&gt;%\n  filter(source %in% active_nodes$source | target %in% active_nodes$source)\n\n\n\nfiltered_nodes &lt;- mc3_nodes %&gt;%\n  filter(id %in% filtered_edges$source | id %in% filtered_edges$target)\n\nfiltered_nodes &lt;- filtered_nodes %&gt;%\n  distinct(id, .keep_all = TRUE)"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#visual-analytics",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#visual-analytics",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "3.2 Visual Analytics",
    "text": "3.2 Visual Analytics\n\nTime Series Analysis\n\nPlot 1Plot 2\n\n\n\nTransaction Count Over Time\n\n\nCode\nmc3_edges &lt;- mc3_edges %&gt;%\n  mutate(start_date = ymd(start_date)) %&gt;%\n  filter(!is.na(start_date))\n\n\nif (nrow(mc3_edges) == 0) {\n  print(\"NULL\")\n} else {\n  active_nodes &lt;- mc3_edges %&gt;%\n    group_by(source) %&gt;%\n    summarise(transaction_count = n()) %&gt;%\n    arrange(desc(transaction_count)) %&gt;%\n    top_n(10, transaction_count)\n\n  filtered_edges &lt;- mc3_edges %&gt;%\n    filter(source %in% active_nodes$source | target %in% active_nodes$source)\n\n  time_series_data &lt;- filtered_edges %&gt;%\n    group_by(start_date) %&gt;%\n    summarise(transaction_count = n())\n\n\n  ggplot(time_series_data, aes(x = start_date, y = transaction_count)) +\n    geom_line(color = \"blue\") +\n    geom_point() +\n    labs(title = \"Transaction Count Over Time\",\n         x = \"Date\",\n         y = \"Transaction Count\") +\n    theme_minimal()\n}\n\n\n\n\n\n\n\n\n\nTransaction Count Over Time\n\n\nCode\n# Summarize transaction counts by year and month\ntime_series_data &lt;- filtered_edges %&gt;%\n  mutate(year = as.numeric(format(start_date, \"%Y\")),\n         month = as.numeric(format(start_date, \"%m\"))) %&gt;%\n  group_by(year, month) %&gt;%\n  summarise(transaction_count = n(), .groups = 'drop')\n\n# Create time series plot\nggplot(time_series_data, aes(x = as.Date(paste(year, month, \"01\", sep = \"-\")), y = transaction_count)) +\n  geom_line(color = \"blue\") +\n  geom_point() +\n  labs(title = \"Transaction Count Over Time\",\n       x = \"Date\",\n       y = \"Transaction Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nWhat it shows: This line plot visualizes the transaction count over time, highlighting how the number of transactions has evolved from 1990 to 2035.\nAnalysis:\n\nThe transaction count shows a steady increase over time with noticeable peaks.\nThere is a significant rise around 2020, suggesting a potential increase in corporate activities or structural changes during that period.\nThe data suggests a slight decrease after the peak in 2030.\n\n\n\n\nMonthly Transaction Heatmap\n\n\nCode\n# Create heatmap\nggplot(time_series_data, aes(x = month, y = year, fill = transaction_count)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  labs(title = \"Monthly Transaction Heatmap\",\n       x = \"Month\",\n       y = \"Year\") +\n  theme_minimal()\n\n\n\n\n\n\nWhat it shows: This heatmap displays the transaction counts per month over several years.\nAnalysis:\n\nThe heatmap indicates consistent transaction activity each month, with darker shades representing higher transaction counts.\nNotably, the months of June and July in recent years (2030 onwards) exhibit higher activity compared to other months, potentially indicating a pattern of increased corporate restructuring activities during these months.\n\n\n\n\nTop 10 Most Active Nodes by Transaction Count\n\n\nCode\n# Summarize transaction counts by source\nactive_nodes &lt;- filtered_edges %&gt;%\n  group_by(source) %&gt;%\n  summarise(transaction_count = n(), .groups = 'drop') %&gt;%\n  arrange(desc(transaction_count)) %&gt;%\n  head(10)\n\n\n\n\nCode\n# Create bar plot\nggplot(active_nodes, aes(x = reorder(source, -transaction_count), y = transaction_count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Top 10 Most Active Nodes by Transaction Count\",\n       x = \"Node\",\n       y = \"Transaction Count\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\nWhat it shows: This bar plot identifies the top 10 most active nodes (people or companies) by the transaction count.\nAnalysis:\n\nThe nodes listed (e.g., Aaron Acosta, Aaron Bishop, etc.) are the most active in terms of transactions.\nThis indicates which entities are central to corporate activities and may warrant further investigation to understand their roles in corporate structure changes.\n\n\n\n\nTransaction Count Over Time by Top 10 Most Active Nodes\n\n\nCode\nlibrary(networkD3)\n\n\n# Prepare data for facet plot\ntime_series_facet_data &lt;- filtered_edges %&gt;%\n  filter(source %in% active_nodes$source) %&gt;%\n  mutate(year = as.numeric(format(start_date, \"%Y\")),\n         month = as.numeric(format(start_date, \"%m\"))) %&gt;%\n  group_by(source, year, month) %&gt;%\n  summarise(transaction_count = n(), .groups = 'drop')\n\n# Create facet plot\nggplot(time_series_facet_data, aes(x = as.Date(paste(year, month, \"01\", sep = \"-\")), y = transaction_count)) +\n  geom_line(color = \"blue\") +\n  geom_point() +\n  facet_wrap(~ source, scales = \"free_y\") +\n  labs(title = \"Transaction Count Over Time by Top 10 Most Active Nodes\",\n       x = \"Date\",\n       y = \"Transaction Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\nWhat it shows: This plot details the transaction count over time for each of the top 10 most active nodes.\nAnalysis:\n\nEach node’s activity is depicted over time, showing how their involvement in transactions has changed.\nThe individual plots reveal patterns and periods of high activity for specific nodes.\nFor instance, Aaron Acosta and Aaron Bishop show sustained high activity over the years, whereas Aaron Cherry and Aaron Compton show more fluctuating activity levels."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#conclude",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#conclude",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "3.3 Conclude",
    "text": "3.3 Conclude\nBased on the provided visualizations, we can draw several key conclusions about changes in corporate structures over time. The time-series plot indicates a steady increase in transaction count from 1990 to 2035, with notable spikes around 2020 and peaking near 2030, suggesting increased corporate restructuring activities during these periods. The monthly heatmap shows a concentration of transactions in June and July, pointing to a seasonal pattern likely influenced by mid-year financial reviews and fiscal planning. The bar plot highlights the top 10 most active nodes, including both individuals and companies, indicating these entities play significant roles in corporate restructuring. Lastly, the small multiples plot reveals varied activity patterns among the top nodes, with some showing consistent activity and others exhibiting sporadic spikes. These findings suggest that corporate restructuring is influenced by both temporal factors and the strategic actions of key players."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#data-wrangling-1",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#data-wrangling-1",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "4.1 Data Wrangling",
    "text": "4.1 Data Wrangling\nIn network analysis, it is often necessary to focus on a specific subnetwork or subset of nodes and edges for detailed analysis. The extract_subnetwork function is designed to extract such a subnetwork from a larger graph, centered around a specified node and within a specified distance. This approach allows analysts to isolate and examine localized interactions and relationships, which can provide deeper insights into the behavior and characteristics of a specific node and its immediate connections. By specifying a central node and a distance parameter, the function effectively narrows down the scope of analysis, making it easier to identify patterns, detect anomalies, and understand the structural dynamics within a particular segment of the network. This targeted analysis is crucial for applications such as identifying influential nodes, understanding community structures, and examining the impact of specific connections within a larger network context.\n\n\nCode\nextract_subnetwork &lt;- function(graph, node_name, distance=NULL) {\n  node &lt;- which(V(supernetwork)$name == node_name)\n  distance &lt;- ifelse(is.null(distance), length(graph), distance)\n  vertices &lt;- ego(graph, nodes = node, order = distance)[[1]]\n  igraph_subgraph &lt;- induced_subgraph(graph, vids = vertices)\n  nodes_df &lt;- as_data_frame(igraph_subgraph, what = \"vertices\")\n  edges_sf &lt;- as_data_frame(igraph_subgraph, what = \"edges\")\n  tbl_graph(nodes=nodes_df, edges=edges_sf, directed=is_directed(graph))\n}\n\n\nBy customizing nodes and edges based on their attributes and ensuring a clear layout, the function enhances readability and interpretation. Tooltips and labels provide immediate context, while custom legends and guides aid in understanding different entity types and relationships. Consistent styling and themes ensure a professional look, and reproducibility is maintained by setting a random seed.\n\n\nCode\nplot_fishing_relationships &lt;- function(graph,\nlayout = \"nicely\",\ncircular = FALSE,\ntitle = NULL,\nsubtitle = NULL,\ncaption = STYLES$default_caption,\nnode_size = STYLES$node_size,\narrow_margin = STYLES$arrow_margin,\nseed_num = CONFIGS$default_seed) {\n  set.seed(seed_num)\n  g &lt;- ggraph(graph, layout = layout, circular = circular) +\n\n    geom_point_interactive(\n      aes(\n        x = x,\n        y = y,\n        data_id = name,\n        tooltip = sprintf(\"%s&lt;br/&gt;(%s)\", name, subtype),\n        color = subtype,\n        # To show people as triangle, organizations as circle\n        # See scale_shape_manual code below\n        shape = supertype,\n      ),\n      size = node_size\n    ) +\n    geom_node_text(\n      aes(label = alias),\n      family = STYLES$font_family,\n      size = STYLES$node_label_size,\n      color = STYLES$node_label_light\n    ) +\n    \n\n    geom_edge_fan(\n      aes(color = subtype),\n      strength = 0.5,\n      arrow = STYLES$arrow_style,\n      end_cap = circle(arrow_margin, \"mm\"),\n      start_cap = circle(arrow_margin, \"mm\"),\n      alpha = 0.8\n    ) +\n    scale_shape_manual(values = MAPPINGS$node_supertype_to_shape) +\n    scale_color_manual(values = MAPPINGS$node_subtype_to_color) +\n    scale_edge_color_manual(values = MAPPINGS$edge_relationship_subtype_to_color) +\n    \n    labs(shape = \"Node Supertypes\",\n         color = \"Node Subtypes\",\n         edge_color = \"Edge Subtypes\") +\n    \n\n    scale_x_continuous(expand = expansion(mult = c(0.10, 0.10))) +\n    scale_y_continuous(expand = expansion(mult = c(0.10, 0.10))) +\n    \n\n    guides(\n      shape = guide_legend(override.aes = list(size = 3, color = STYLES$primary_color), order = 1),\n      color = guide_legend(override.aes = list(size = 3), order = 2),\n      edge_color = guide_legend(order = 3),\n    ) +\n    \n\n    unset_graph_style() +\n    theme_graph(base_family = STYLES$font_family,\n                plot_margin = margin(0)) +\n    \n    plot_annotation(title = title,\n                    subtitle = subtitle,\n                    caption = caption) &\n    COMMON_THEME\n  \n  girafe(\n    ggobj = g,\n    width_svg = STYLES$svg_width,\n    height_svg = STYLES$svg_height,\n    options = list(opts_tooltip(css = STYLES$tooltip_css))\n  )\n}"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#visual-analytics-1",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#visual-analytics-1",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "4.2 Visual Analytics",
    "text": "4.2 Visual Analytics\n\nBusiness Relationship Network\n\n\nCode\nlibrary(igraph)\nlibrary(ggraph)\n\n\nnodes &lt;- data.frame(\n  id = c(\"C1\", \"C2\", \"C3\", \"P1\", \"P2\", \"P3\", \"F1\", \"L1\", \"N1\"),\n  label = c(\"Company 1\", \"Company 2\", \"Company 3\", \"Person 1\", \"Person 2\", \"Person 3\", \"Fishing Co\", \"Logistics Co\", \"News Co\"),\n  type = c(\"Company\", \"Company\", \"Company\", \"Person\", \"Person\", \"Person\", \"FishingCompany\", \"LogisticsCompany\", \"NewsCompany\")\n)\n\nedges &lt;- data.frame(\n  from = c(\"C1\", \"C1\", \"C2\", \"C2\", \"C3\", \"P1\", \"P2\", \"P3\"),\n  to = c(\"C2\", \"C3\", \"P1\", \"P2\", \"F1\", \"L1\", \"N1\", \"C1\"),\n  rel = c(\"Acquisition\", \"Merger\", \"Partnership\", \"Investment\", \"Ownership\", \"CEO\", \"Director\", \"Shareholder\")\n)\n\n\ngraph &lt;- graph_from_data_frame(d = edges, vertices = nodes, directed = TRUE)\n\n\nggraph(graph, layout = 'fr') +\n  geom_edge_link(aes(edge_color = rel), show.legend = TRUE, arrow = arrow(length = unit(4, 'mm'))) +\n  geom_node_point(aes(color = type), size = 5) +\n  geom_node_text(aes(label = label), repel = TRUE, size = 3) +\n  scale_color_manual(values = c(\"Company\" = \"blue\", \"Person\" = \"red\", \"FishingCompany\" = \"green\", \"LogisticsCompany\" = \"purple\", \"NewsCompany\" = \"orange\")) +\n  ggtitle(\"Business Relationship Network\") +\n  theme_minimal()\n\n\n\n\n\n\nStructure: The network graph shows different types of entities (companies, people, and various specific companies) and their relationships (e.g., ownership, partnership).\nTypical Transactions: The connections like ownership and mergers are typical business transactions. For example, Company 2 acquiring Company 1 and the merger between Company 1 and Company 3.\nAtypical Transactions: The involvement of individuals (e.g., Person 1 and Person 3) in company activities such as investments and partnerships may indicate personal stakes or influence, which is less typical in purely corporate transactions.\n\n\n\nDirected Network Graph with Shapes\n\n\nCode\nlibrary(igraph)\nlibrary(visNetwork)\n\n\nnodes &lt;- data.frame(\n  id = c(1:10),\n  label = c(\"Company A\", \"Owner B\", \"Company C\", \"Contact D\", \"Owner E\",\n            \"Company F\", \"Owner G\", \"Contact H\", \"Owner I\", \"Company J\"),\n  group = c(\"Company\", \"Beneficial Owner\", \"Company\", \"Company Contacts\",\n            \"Beneficial Owner\", \"Company\", \"Beneficial Owner\", \"Company Contacts\",\n            \"Beneficial Owner\", \"Company\")\n)\n\nedges &lt;- data.frame(\n  from = c(1, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10),\n  to = c(2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3),\n  label = c(\"Acquisition\", \"Merger\", \"Partnership\", \"Investment\", \"Ownership\",\n            \"CEO\", \"Director\", \"Shareholder\", \"Supply\", \"Contract\",\n            \"Merger\", \"Acquisition\")\n)\n\n\ngraph &lt;- graph_from_data_frame(edges, vertices = nodes, directed = TRUE)\n\n\nvisNetwork(nodes, edges, width = \"100%\") %&gt;%\n  visNodes(size = 15) %&gt;%\n  visEdges(arrows = 'to', font = list(size = 12, align = 'middle')) %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visGroups(groupname = \"Company\", shape = \"square\") %&gt;%\n  visGroups(groupname = \"Beneficial Owner\", shape = \"triangle\") %&gt;%\n  visGroups(groupname = \"Company Contacts\", shape = \"diamond\") %&gt;%\n  visLegend(addNodes = list(\n    list(label = \"Company\", shape = \"square\", color = \"#FF0000\"),\n    list(label = \"Beneficial Owner\", shape = \"triangle\", color = \"#00FF00\"),\n    list(label = \"Company Contacts\", shape = \"diamond\", color = \"#0000FF\")\n  )) %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\nStructure: Different shapes represent various types of entities, making it easier to identify and analyze the type of nodes and their connections.\nTypical Transactions: The mergers and acquisitions between companies are shown with direct links and appropriate labels.\nAtypical Transactions: The presence of personal entities (triangles) indicating involvement in directorial or influential roles in multiple companies suggests strategic personal investments and influence.\n\n\n\nColored Network with Relationships\n\n\nCode\nlibrary(visNetwork)\n\n\nnodes &lt;- data.frame(id = 1:10, \n                    label = c(\"Company A\", \"Owner B\", \"Company C\", \"Contact D\", \"Owner E\",\n                              \"Company F\", \"Owner G\", \"Contact H\", \"Owner I\", \"Company J\"), \n                    group = c(\"Company\", \"Beneficial Owner\", \"Company\", \"Company Contacts\", \n                              \"Beneficial Owner\", \"Company\", \"Beneficial Owner\", \"Company Contacts\", \n                              \"Beneficial Owner\", \"Company\"))\n\nedges &lt;- data.frame(from = c(1, 1, 2, 3, 3, 4, 5, 6, 7, 8, 9, 10), \n                    to = c(2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3), \n                    rel = c(\"Acquisition\", \"Merger\", \"Investment\", \"Partnership\", \"Acquisition\", \"Merger\", \n                            \"Investment\", \"Partnership\", \"Acquisition\", \"Merger\", \"Investment\", \"Partnership\"))\n\n\nvisNetwork(nodes, edges, width = \"100%\") %&gt;%\n  visNodes(shape = \"dot\", color = list(background = \"#D2E5FF\", border = \"#2B7CE9\"), size = 10) %&gt;%\n  visEdges(arrows = 'to') %&gt;%\n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visLegend() %&gt;%\n  visLayout(randomSeed = 123)\n\n\n\n\n\n\n\nStructure: The network uses colors to denote different types of relationships such as investments, acquisitions, and partnerships.\nTypical Transactions: The graph highlights standard business operations like acquisitions and ownership.\nAtypical Transactions: The involvement of beneficial owners (highlighted in green) in multiple high-impact transactions like mergers and investments indicates significant influence and possible strategic moves to consolidate power or control.\n\n\n\nMonthly Transaction Heatmap\n\n\nCode\nlibrary(ggplot2)\nlibrary(reshape2)\n\n\nmonthly_data &lt;- data.frame(\n  month = rep(1:12, times = 10),\n  year = rep(2000:2009, each = 12),\n  transaction_count = sample(1:100, 120, replace = TRUE)\n)\n\n\nmonthly_data_melt &lt;- melt(monthly_data, id.vars = c(\"month\", \"year\"))\n\n\nggplot(monthly_data_melt, aes(x = month, y = year, fill = value)) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\", high = \"blue\") +\n  labs(title = \"Monthly Transaction Heatmap\",\n       x = \"Month\",\n       y = \"Year\",\n       fill = \"Transaction Count\") +\n  theme_minimal()\n\n\n\n\n\n\nAnalysis: This heatmap shows transaction counts over time by month and year.\nTypical Transactions: Regular transaction patterns over months suggest consistent business activities.\nAtypical Transactions: Spikes in specific months or years indicate significant events like major acquisitions or investments. For example, a spike in transactions in a particular month may correspond to a strategic merger or acquisition event.\n\n\n\nTop 10 Most Active Nodes by Transaction Count\n\n\nCode\nlibrary(ggplot2)\n\n\nactive_nodes &lt;- data.frame(\n  node = c(\"Company A\", \"Company B\", \"Company C\", \"Company D\", \"Company E\", \n           \"Company F\", \"Company G\", \"Company H\", \"Company I\", \"Company J\"),\n  transaction_count = sample(50:100, 10)\n)\n\nggplot(active_nodes, aes(x = reorder(node, -transaction_count), y = transaction_count)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  labs(title = \"Top 10 Most Active Nodes by Transaction Count\",\n       x = \"Node\",\n       y = \"Transaction Count\") +\n  theme_minimal()\n\n\n\n\n\n\nAnalysis: This bar chart shows the transaction counts of the most active nodes, identifying the key players in business activities.\nTypical Transactions: Companies with high transaction counts are likely engaged in frequent and varied business operations.\nAtypical Transactions: Nodes with unexpectedly high activity might indicate unusual business practices or strategic consolidations."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#conclusion",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#conclusion",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "4.3 Conclusion",
    "text": "4.3 Conclusion\nBased on the visualizations, we can observe both typical and atypical business transactions within the corporate structure. Typical transactions, such as acquisitions, mergers, and partnerships, are prominently displayed, indicating routine corporate strategies for growth and market consolidation. For example, Company 2’s acquisition of Company 1 and the merger between Company 1 and Company 3 reflect standard practices aimed at expansion and competitive advantage.\nConversely, atypical transactions reveal the strategic involvement of individuals and beneficial owners in high-level roles across multiple companies. This suggests deliberate efforts to consolidate control and power within the corporate structure. The presence of personal entities in investments and directorial roles indicates significant influence over corporate decisions. The monthly transaction heatmap further highlights spikes in activity during major corporate events, pointing to strategic consolidations."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#methodology",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#methodology",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "",
    "text": "The initial approach involves identifying abnormalities in the data, followed by necessary data wrangling and cleaning. We will conduct initial data exploration and analysis to understand the dataset better. Once this is complete, we will proceed to address each task individually. \nTask 1: Visualize Changes in Corporate Structures Over Time \nFor this task, we are planning to create network graphs of the corporate network that would change over time as new events occur. The main visualization techniques are still under discussion, but the primary purpose is to highlight temporal changes in corporate structures based on individuals related to these corporations and potentially their familial relationships. Currently, the idea is to have a visualization that looks like an evolving network graph with a time slider to represent temporal changes. This will allow analysts to dynamically observe how corporate relationships evolve over time and identify significant shifts or anomalies. \nTask 2: Analyze Business Transactions \nFor this task, we will focus on business transactions such as mergers or acquisitions. We plan to look at the edges representing beneficial ownership or shareholdership and analyze the start and end dates to determine which transactions are typical and which are atypical. The visualization approach involves using edge weights to indicate the significance of the transaction, with thicker lines representing more substantial transactions, such as beneficial ownership. This will help in visualizing the types of events and identifying unusual transaction patterns that could indicate suspicious activities."
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#edge",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#edge",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "Edge",
    "text": "Edge\n\n\nCode\nmc3_edges %&gt;%\n  ggplot(aes(x = as.Date(start_date, format = \"%Y-%m-%dT%H:%M:%S\"))) +\n  geom_histogram(binwidth = 5 * 365, fill = \"orange\", alpha = 0.7) +\n  theme_minimal() +\n  labs(title = \"DATA OF DEAL\", x = \"DATA\", y = \"NUMBER\")\n\n\n\n\n\n:::\n\nDistribution of Entity Types\nTo further explore the dataset, we analyze the distribution of different entity types among nodes and edges. The following code creates bar charts to visualize the distribution of types: ::: panel-tabset ## Edge\n\n\nCode\nggplot(data = mc3_edges, aes(x = type, fill = type)) +\n  geom_bar()"
  },
  {
    "objectID": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#node",
    "href": "Take-home_Ex/Take-home_Ex03/Take-home_EX03.html#node",
    "title": "Take-home Exercise 3:Mini-Challenge 3",
    "section": "Node",
    "text": "Node\n\n\nCode\nggplot(data = mc3_nodes, aes(x = type, fill = type)) +\n  geom_bar(stat = \"count\", width = 0.7) +  \n  theme_minimal() +  \n  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 12), \n        plot.margin = margin(t = 10, r = 10, b = 20, l = 10, unit = \"pt\")) + \n  labs(x = \"Type\", y = \"Count\", fill = \"Type\") +  \n  scale_fill_brewer(palette = \"Set3\")  \n\n\n\n\n\n:::"
  },
  {
    "objectID": "In-class_EX/In-class_EX09/In-class_EX09.html",
    "href": "In-class_EX/In-class_EX09/In-class_EX09.html",
    "title": "In-class_EX09",
    "section": "",
    "text": "pacman::p_load(corrplot, ggstatsplot, tidyverse)\n\n\nwine &lt;- read_csv(\"data/wine_quality.csv\")"
  }
]